
# -*- coding: utf-8 -*-
"""
–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è v5.6

–≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ä–∞–∑–≤–µ–¥–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö (EDA),
–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö A/B‚Äë—Ç–µ—Å—Ç–æ–≤ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º
—Ä—è–¥–∞–º. –í –≤–µ—Ä—Å–∏–∏ 5.6 –¥–æ–±–∞–≤–ª–µ–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É–ª—É—á—à–µ–Ω–∏–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö,
–ø–æ–¥–¥–µ—Ä–∂–∫–µ –±–æ–ª—å—à–µ–≥–æ —á–∏—Å–ª–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤, —É–ª—É—á—à–µ–Ω—ã –æ—Ç—á—ë—Ç—ã –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã
–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–¥–±–æ—Ä—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

–ö–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è v5.6:
* –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å
  —á–∏—Å–ª–∞ –∏ –¥–∞—Ç—ã –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö), –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤ CSV, Excel,
  Parquet –∏ Feather.
* –£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä Excel‚Äë–æ—Ç—á—ë—Ç–æ–≤: –¥–æ–±–∞–≤–ª–µ–Ω —Ç–∏—Ç—É–ª—å–Ω—ã–π –ª–∏—Å—Ç (Cover),
  –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–¥–∞—á–µ `None` –∏–ª–∏
  –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤–º–µ—Å—Ç–æ DataFrame.
* –ò–º–ø—É—Ç–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π –∏–ª–∏
  –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º, –∞ —Ç–∞–∫–∂–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –≤–∏–Ω–∑–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ —É—Ä–æ–≤–Ω–µ
  —Å—Ç–æ–ª–±—Ü–æ–≤.
* –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π A/B‚Äë–º–æ–¥—É–ª—å: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Mann‚ÄìWhitney U –∏ z‚Äë—Ç–µ—Å—Ç–∞ –¥–ª—è
  –ø—Ä–æ–ø–æ—Ä—Ü–∏–π, –≤—ã–≤–æ–¥ —Ä–∞–∑–º–µ—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞, –±–∏–∑–Ω–µ—Å‚Äë—Å–≤–æ–¥–∫–∞.
* –ú–æ–¥—É–ª—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: –¥–æ–±–∞–≤–ª–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ ARIMA
  –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –ø–µ—Ä–µ–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (auto ARIMA) –∏ –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
  –∫—Ä–æ—Å—Å‚Äë–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (MAE, MAPE, RMSE).
* –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–µ—Å–µ—Ç—ã –¥–ª—è –≤—ã–±–æ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫: –º–æ–∂–Ω–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –≤—ã–±—Ä–∞—Ç—å –≤—Å–µ
  —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤, –ª–∏–±–æ —Ç–æ–ø‚Äë10 –Ω–∞–∏–±–æ–ª–µ–µ
  –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É –≤–∞—Ä–∏–∞—Ü–∏–∏.
* –û–¥–∏–Ω –∫–ª–∏–∫ ‚Äî –æ–¥–∏–Ω –æ—Ç—á—ë—Ç: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∫–∞—á–∞—Ç—å zip‚Äë–∞—Ä—Ö–∏–≤ —Å HTML‚Äë –∏
  Excel‚Äë–æ—Ç—á—ë—Ç–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –∫–æ–Ω—Ñ–∏–≥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ (filters + –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏).
–ó–∞–ø—É—Å–∫: streamlit run stats_lab_app_v5_5.py
"""
import streamlit as st
import warnings
import pandas as pd
import numpy as np
import json
from io import BytesIO

# pptx –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤ PowerPoint
try:
    from pptx import Presentation  # type: ignore
    from pptx.util import Inches, Pt  # type: ignore
except Exception:
    Presentation = None  # –µ—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –æ—Ç—á—ë—Ç—ã PPTX –Ω–µ –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã
import math
# –í–Ω–µ—à–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: scipy, plotly, statsmodels
# –ò—Ö –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install scipy plotly statsmodels
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
try:
    from statsmodels.tsa.seasonal import STL
except Exception:
    STL = None  # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –Ω–∏–∂–µ

try:
    from statsmodels.tsa.arima.model import ARIMA
except Exception:
    ARIMA = None  # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ

# –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ pmdarima –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–≤—Ç–æ-ARIMA. –ï—Å–ª–∏ –Ω–µ—Ç,
# –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –±—É–¥–µ—Ç False. –ù–æ –∞–≤—Ç–æ‚ÄëARIMA —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏,
# –ø–æ—ç—Ç–æ–º—É –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å pmdarima –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–∞.
try:
    import pmdarima  # type: ignore
    PMDARIMA_AVAILABLE = True
except Exception:
    PMDARIMA_AVAILABLE = False

# --------------
# ------------------------- UI STYLE -------------------------
def inject_custom_css():
    st.markdown(
        """
        <style>
        .business-summary {
            border-radius: 12px;
            border: 1px solid #d1fae5;
            background-color: #ecfdf5;
            padding: .8rem 1rem;
            font-size: .95rem;
            white-space: pre-line;
        }

        .stTabs [data-baseweb="tab"] {
            border-radius: 10px;
        }
        .stTabs [aria-selected="true"] {
            background: #eef4ff !important;
        }

        .step-badge {
            padding: 0.35rem 0.7rem;
            border-radius: 999px;
            background-color: #eef2ff;
            color: #1f2933;
            font-size: 0.8rem;
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
        }
        .step-badge span {
            font-weight: 600;
        }
        .step-badge .muted {
            opacity: .8;
            font-weight: 400;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )


METHOD_INFO = {
    "mean": {"name": "–°—Ä–µ–¥–Ω–µ–µ", "description": "–ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ.", "when": "–°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è."},
    "median": {"name": "–ú–µ–¥–∏–∞–Ω–∞", "description": "–°–µ—Ä–µ–¥–∏–Ω–∞ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π.", "when": "–ï—Å—Ç—å –≤—ã–±—Ä–æ—Å—ã/—Å–∫–æ—Å."},
    "mode": {"name": "–ú–æ–¥–∞", "description": "–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.", "when": "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ/–¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."},
    "std": {"name": "Std", "description": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ.", "when": "–û—Ü–µ–Ω–∫–∞ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏."},
    "cv": {"name": "CV", "description": "–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å (std/mean).", "when": "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏."},
    "iqr": {"name": "IQR", "description": "–ú–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö.", "when": "–£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º."},
}


# ------------------------- HELPERS -------------------------
@st.cache_data
def describe_basic_stats(series: pd.Series):
    col = series.dropna()
    res = {
        "count": int(col.count()),
        "mean": float(col.mean()) if col.count() else np.nan,
        "median": float(col.median()) if col.count() else np.nan,
        "min": float(col.min()) if col.count() else np.nan,
        "max": float(col.max()) if col.count() else np.nan,
        "range": float(col.max() - col.min()) if col.count() else np.nan,
        "var": float(col.var(ddof=1)) if col.count() > 1 else np.nan,
        "std": float(col.std(ddof=1)) if col.count() > 1 else np.nan,
        "q1": float(col.quantile(0.25)) if col.count() else np.nan,
        "q3": float(col.quantile(0.75)) if col.count() else np.nan,
    }
    res["iqr"] = res["q3"] - res["q1"] if not np.isnan(res["q3"]) and not np.isnan(res["q1"]) else np.nan
    res["cv"] = (res["std"] / res["mean"]) if res["mean"] not in (0, np.nan) and not np.isnan(res["mean"]) else np.nan

    try:
        mode_values = stats.mode(col, keepdims=True)
        res["mode"] = float(mode_values.mode[0]) if len(mode_values.mode) > 0 else np.nan
    except Exception:
        res["mode"] = np.nan

    res["skewness"] = float(stats.skew(col, bias=False)) if len(col) > 2 else np.nan
    res["kurtosis"] = float(stats.kurtosis(col, fisher=True, bias=False)) if len(col) > 3 else np.nan
    for p in (5, 10, 25, 50, 75, 90, 95):
        res[f"p{p}"] = float(col.quantile(p/100)) if len(col) else np.nan
    return res


def detect_outliers_iqr(series, k=1.5):
    col = series.dropna()
    if len(col) == 0:
        return pd.Series(False, index=series.index), np.nan, np.nan, np.nan
    q1, q3 = col.quantile(0.25), col.quantile(0.75)
    iqr = q3 - q1
    lower, upper = q1 - k * iqr, q3 + k * iqr
    mask = (series < lower) | (series > upper)
    return mask, float(lower), float(upper), float(iqr)


def detect_outliers_z(series, z_thresh=3.0):
    col = series.dropna()
    if len(col) == 0 or col.std(ddof=0) == 0:
        return pd.Series(False, index=series.index)
    z = (series - col.mean()) / col.std(ddof=0)
    return z.abs() > z_thresh

# ------------------------- IMPUTATION & CLEANING HELPERS -------------------------
def winsorize_series(series: pd.Series, lower_pct: float = 0.01, upper_pct: float = 0.99) -> pd.Series:
    """
    –ö–ª–∏–ø–ø–∏–Ω–≥ (–≤–∏–Ω–∑–æ—Ä–∏–∑–∞—Ü–∏—è) –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Å–µ—Ä–∏–∏ –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º.
    –ó–Ω–∞—á–µ–Ω–∏—è –Ω–∏–∂–µ lower_pct –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –∫–≤–∞–Ω—Ç–∏–ª—å lower_pct,
    –∞ –≤—ã—à–µ upper_pct ‚Äî –Ω–∞ –∫–≤–∞–Ω—Ç–∏–ª—å upper_pct.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        series: –∏—Å—Ö–æ–¥–Ω—ã–π —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü;
        lower_pct: –Ω–∏–∂–Ω–∏–π –∫–≤–∞–Ω—Ç–∏–ª—å (0‚Äì1);
        upper_pct: –≤–µ—Ä—Ö–Ω–∏–π –∫–≤–∞–Ω—Ç–∏–ª—å (0‚Äì1).

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        Series —Å –ø–æ–¥—Ä–µ–∑–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
    """
    # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ NaN –∏–ª–∏ –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if series.dropna().empty:
        return series
    low = series.quantile(lower_pct)
    high = series.quantile(upper_pct)
    return series.clip(lower=low, upper=high)

def impute_dataframe(df: pd.DataFrame, columns: list[str], strategy: str = "median") -> pd.DataFrame:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–º–ø—É—Ç–∞—Ü–∏—é –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
      - 'median': –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –º–µ–¥–∏–∞–Ω–æ–π (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫).
      - 'most_frequent': –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π DataFrame —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è).
    """
    new_df = df.copy()
    for c in columns:
        if strategy == "median":
            try:
                med = new_df[c].median()
                new_df[c] = new_df[c].fillna(med)
            except Exception:
                pass
        elif strategy == "most_frequent":
            try:
                mode = new_df[c].mode()
                if not mode.empty:
                    new_df[c] = new_df[c].fillna(mode.iloc[0])
            except Exception:
                pass
    return new_df

# -----------------------------------------------------------------------------
# –¢–∏–ø–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
# -----------------------------------------------------------------------------
def auto_coerce_dataframe_types(df: pd.DataFrame, date_threshold: float = 0.8, num_threshold: float = 0.8) -> pd.DataFrame:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —á–∏—Å–ª–æ–≤—ã–µ –∏–ª–∏ datetime, –µ—Å–ª–∏
    –ø–æ–¥–∞–≤–ª—è—é—â–µ–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ —Ç–∞–∫–∏–µ —Ç–∏–ø—ã.

    :param df: –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame
    :param date_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π, —á—Ç–æ–±—ã –∫–æ–ª–æ–Ω–∫–∞
        —Å—á–∏—Ç–∞–ª–∞—Å—å –¥–∞—Ç–æ–π
    :param num_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π, —á—Ç–æ–±—ã –∫–æ–ª–æ–Ω–∫–∞
        —Å—á–∏—Ç–∞–ª–∞—Å—å —á–∏—Å–ª–æ–≤–æ–π
    :return: –Ω–æ–≤—ã–π DataFrame —Å –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ (–æ—Ä–∏–≥–∏–Ω–∞–ª –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è)
    """
    new_df = df.copy()
    for col in new_df.columns:
        s = new_df[col]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ object/string –∫–æ–ª–æ–Ω–∫–∏
        if not pd.api.types.is_object_dtype(s):
            continue
        try:
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É. –ï—Å–ª–∏ –≤ —á–∏—Å–ª–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–∞–ø—è—Ç—ã–µ –∫–∞–∫
            # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏, –∑–∞–º–µ–Ω—è–µ–º –∏—Ö –Ω–∞ —Ç–æ—á–∫–∏.
            str_series = s.astype(str).str.replace(",", ".", regex=False)
            numeric_converted = pd.to_numeric(str_series, errors="coerce")
            ratio_num = numeric_converted.notna().sum() / len(numeric_converted) if len(numeric_converted) > 0 else 0.0
            if ratio_num >= num_threshold:
                new_df[col] = numeric_converted
                continue
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –¥–∞—Ç–µ (–ø–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–µ)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UserWarning)
                date_converted = pd.to_datetime(s, errors="coerce")
            ratio_date = date_converted.notna().sum() / len(date_converted) if len(date_converted) > 0 else 0.0
            if ratio_date >= date_threshold:
                new_df[col] = date_converted
                continue
        except Exception:
            pass
    return new_df


def normality_test(series):
    col = series.dropna()
    if len(col) < 3:
        return None
    sample = col.sample(5000, random_state=42) if len(col) > 5000 else col
    stat, p_value = stats.shapiro(sample)
    return {"statistic": float(stat), "p_value": float(p_value), "n_used": int(len(sample))}


def cohen_d(x, y):
    x, y = np.array(x), np.array(y)
    nx, ny = len(x), len(y)
    if nx < 2 or ny < 2:
        return np.nan
    sx, sy = x.std(ddof=1), y.std(ddof=1)
    sp = np.sqrt(((nx - 1) * sx**2 + (ny - 1) * sy**2) / (nx + ny - 2))
    return float((x.mean() - y.mean()) / sp) if sp else np.nan


def detect_id_columns(df, threshold_ratio=0.9, min_unique=10):
    n = len(df)
    if n == 0: return []
    out = []
    nunq = df.nunique(dropna=False)
    for c, k in nunq.items():
        if k >= min_unique and k / n >= threshold_ratio:
            out.append(c)
    return out


@st.cache_data
def compute_corr_matrix_cached(df: pd.DataFrame, cols: tuple, method: str = "pearson"):
    return df[list(cols)].corr(method=method)


@st.cache_data
def compute_data_quality_table(df: pd.DataFrame):
    n = len(df)
    rows = []
    for col in df.columns:
        s = df[col]
        n_missing = int(s.isna().sum())
        missing_pct = float((n_missing / n * 100) if n else 0.0)
        n_unique = int(s.nunique(dropna=True))
        is_constant = bool(n_unique <= 1)
        base_dtype = str(s.dtype)
        type_suggestion: str | None = None
        mixed_types = False
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∏–ª–∏ object –∫–æ–ª–æ–Ω–∫–∏
        if pd.api.types.is_object_dtype(s):
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –∫ —á–∏—Å–ª—É
                str_series = s.astype(str).str.replace(",", ".", regex=False)
                num_converted = pd.to_numeric(str_series, errors="coerce")
                ratio_num = num_converted.notna().sum() / len(num_converted) if len(num_converted) > 0 else 0.0
                # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –¥–∞—Ç–µ (–ø–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–µ)
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", UserWarning)
                    date_converted = pd.to_datetime(s, errors="coerce")
                ratio_date = date_converted.notna().sum() / len(date_converted) if len(date_converted) > 0 else 0.0
                # –ï—Å–ª–∏ –æ–¥–Ω–∞ –∏–∑ –¥–æ–ª–µ–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–µ–ª–∏–∫–∞, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø—Ä–∏–≤–µ—Å—Ç–∏ —Ç–∏–ø
                if ratio_num >= 0.8:
                    type_suggestion = "numeric"
                elif ratio_date >= 0.8:
                    type_suggestion = "datetime"
                # –ï—Å–ª–∏ —á–∞—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∞—Å—å, –Ω–æ –º–µ–Ω–µ–µ 80%, —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å–º–µ—à–∞–Ω–Ω–æ–π
                if (0.0 < ratio_num < 0.8) or (0.0 < ratio_date < 0.8):
                    mixed_types = True
            except Exception:
                pass
        rows.append(
            {
                "column": col,
                "dtype": base_dtype,
                "n_missing": n_missing,
                "missing_%": missing_pct,
                "n_unique": n_unique,
                "is_constant": is_constant,
                "mixed_types": mixed_types,
                "type_suggestion": type_suggestion,
            }
        )
    return pd.DataFrame(rows)


def get_strong_correlations(corr_matrix, threshold=0.7):
    rec = []
    cols = corr_matrix.columns
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            r = corr_matrix.iloc[i, j]
            if np.isnan(r): continue
            if abs(r) >= threshold:
                rec.append({"feature_1": cols[i], "feature_2": cols[j], "r": float(r), "abs_r": float(abs(r))})
    return pd.DataFrame(rec).sort_values("abs_r", ascending=False) if rec else pd.DataFrame(
        columns=["feature_1", "feature_2", "r", "abs_r"]
    )


def maybe_downsample_xy(x, y, max_points=10000):
    if len(x) <= max_points: return x, y
    idx = np.random.choice(len(x), size=max_points, replace=False)
    return (x.iloc[idx] if isinstance(x, pd.Series) else x[idx],
            y.iloc[idx] if isinstance(y, pd.Series) else y[idx])


@st.cache_data
def compute_acf(series, max_lag):
    x = series.dropna().values
    n = len(x)
    if n == 0:
        return np.arange(max_lag + 1), np.full(max_lag + 1, np.nan)
    x = x - x.mean()
    denom = np.dot(x, x)
    if denom == 0:
        return np.arange(max_lag + 1), np.full(max_lag + 1, np.nan)
    acf_vals = [1.0]
    for lag in range(1, max_lag + 1):
        num = np.dot(x[:-lag], x[lag:])
        acf_vals.append(num / denom)
    return np.arange(max_lag + 1), np.array(acf_vals)


def generate_ts_features(df, date_col, value_col, window=7, spike_thresh_pct=50.0):
    data = df[[date_col, value_col]].dropna().copy().sort_values(date_col)
    data[date_col] = pd.to_datetime(data[date_col])

    data["lag_1"] = data[value_col].shift(1)
    data["diff_1"] = data[value_col] - data["lag_1"]
    data["pct_change_1"] = data[value_col].pct_change() * 100.0
    data["rolling_mean_window"] = data[value_col].rolling(window=window, min_periods=1).mean()
    data["rolling_std_window"] = data[value_col].rolling(window=window, min_periods=1).std(ddof=1)
    data["rolling_cv_window"] = (data["rolling_std_window"] / data["rolling_mean_window"]).replace([np.inf, -np.inf], np.nan)
    data["spike_flag"] = data["pct_change_1"].abs() > spike_thresh_pct

    y = data[value_col].values
    x = np.arange(len(y))
    slope, intercept = (np.polyfit(x, y, 1) if (len(y) >= 2 and len(np.unique(y)) > 1) else (np.nan, np.nan))

    global_features = {
        "n_points": int(len(y)),
        "mean": float(np.nanmean(y)) if len(y) > 0 else np.nan,
        "std": float(np.nanstd(y, ddof=1)) if len(y) > 1 else np.nan,
        "cv": float(np.nanstd(y, ddof=1) / np.nanmean(y)) if len(y) > 1 and np.nanmean(y) != 0 else np.nan,
        "slope_trend": float(slope),
        "intercept_trend": float(intercept),
        "first_value": float(y[0]) if len(y) > 0 else np.nan,
        "last_value": float(y[-1]) if len(y) > 0 else np.nan,
        "change_abs": float(y[-1] - y[0]) if len(y) > 1 else np.nan,
        "change_pct": float((y[-1] - y[0]) / y[0] * 100.0) if len(y) > 1 and y[0] != 0 else np.nan,
    }
    return data, global_features


def plot_ts_plotly(df_ts, date_col, value_col, method, window=7):
    data = df_ts[[date_col, value_col]].dropna().copy().sort_values(date_col)
    data[date_col] = pd.to_datetime(data[date_col])

    plot_df = pd.DataFrame({"date": data[date_col], "value": data[value_col]})
    if method == "–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ":
        plot_df["smoothed"] = plot_df["value"].rolling(window=window, min_periods=1).mean()
    elif method == "EWMA":
        plot_df["smoothed"] = plot_df["value"].ewm(span=window, adjust=False).mean()
    elif method == "–°–∫–æ–ª—å–∑—è—â–∞—è –º–µ–¥–∏–∞–Ω–∞":
        plot_df["smoothed"] = plot_df["value"].rolling(window=window, min_periods=1).median()
    else:
        plot_df["smoothed"] = np.nan

    fig = px.line(plot_df, x="date", y=["value", "smoothed"], labels={"date": "–î–∞—Ç–∞"})
    fig.update_layout(legend_title_text="–°–µ—Ä–∏–∏", hovermode="x unified", margin=dict(t=40, r=20, b=40, l=40))
    return fig


# ------------------------- BUSINESS TEXT -------------------------

def ts_forecast_arima(df, date_col, value_col, horizon, order=(1, 1, 1)):
    """
    –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥–Ω–æ–∑ ARIMA –¥–ª—è –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–æ–≥–Ω–æ–∑ —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º.
    """
    if ARIMA is None:
        raise ImportError("ARIMA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ statsmodels>=0.12.")

    data = df[[date_col, value_col]].dropna().copy()
    if data.empty:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞.")

    data[date_col] = pd.to_datetime(data[date_col])
    data = data.sort_values(date_col).set_index(date_col)
    y = pd.to_numeric(data[value_col], errors="coerce").dropna()
    if len(y) < 5:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ (–º–∏–Ω–∏–º—É–º 5) –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è ARIMA-–ø—Ä–æ–≥–Ω–æ–∑–∞.")

    model = ARIMA(y, order=order)
    res = model.fit()

    forecast_res = res.get_forecast(steps=int(horizon))
    mean_forecast = forecast_res.predicted_mean
    conf_int = forecast_res.conf_int(alpha=0.1)  # 90% –î–ò

    fc_df = pd.DataFrame(
        {
            "date": mean_forecast.index.to_timestamp() if hasattr(mean_forecast.index, "to_timestamp") else mean_forecast.index,
            "forecast": mean_forecast.values,
            "lower": conf_int.iloc[:, 0].values,
            "upper": conf_int.iloc[:, 1].values,
        }
    )

    hist_df = y.reset_index().rename(columns={value_col: "value", date_col: "date"})
    return hist_df, fc_df

# -----------------------------------------------------------------------------
# Auto-ARIMA: –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
# -----------------------------------------------------------------------------
def ts_forecast_auto_arima(
    df: pd.DataFrame,
    date_col: str,
    value_col: str,
    horizon: int,
    max_p: int = 2,
    max_d: int = 1,
    max_q: int = 2,
) -> tuple[pd.DataFrame, pd.DataFrame, tuple[int, int, int]]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ARIMA –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é BIC.
    –ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ (p,d,q) –≤ –∑–∞–¥–∞–Ω–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
    —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º BIC –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ
    –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ø–µ—Ä–≤–∞—è. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ
    –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–æ–≥–Ω–æ–∑ –∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫.

    :param df: –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä—è–¥–æ–º
    :param date_col: –∏–º—è —Å—Ç–æ–ª–±—Ü–∞ –¥–∞—Ç
    :param value_col: –∏–º—è —Å—Ç–æ–ª–±—Ü–∞ –∑–Ω–∞—á–µ–Ω–∏–π
    :param horizon: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –ø—Ä–æ–≥–Ω–æ–∑–∞
    :param max_p: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ p
    :param max_d: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ d
    :param max_q: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ q
    :returns: (hist_df, forecast_df, best_order)
    """
    if ARIMA is None:
        raise ImportError("ARIMA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ statsmodels>=0.12.")
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data = df[[date_col, value_col]].dropna().copy()
    if data.empty:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞.")
    data[date_col] = pd.to_datetime(data[date_col])
    data = data.sort_values(date_col).set_index(date_col)
    y = pd.to_numeric(data[value_col], errors="coerce").dropna()
    if len(y) < 10:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–µ–∫ (–º–∏–Ω–∏–º—É–º 10) –¥–ª—è –∞–≤—Ç–æ-ARIMA.")
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç (—Ö–æ—Ç—è BIC –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—Å—ë–º –Ω–∞–±–æ—Ä–µ)
    best_bic = np.inf
    best_order: tuple[int, int, int] = (1, 1, 1)
    best_res = None
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º d, —Ç.–∫. –ø–µ—Ä–µ–±–æ—Ä d>2 —Ä–µ–¥–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω
    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é (0,0,0)
                if p == 0 and d == 0 and q == 0:
                    continue
                try:
                    model = ARIMA(y, order=(p, d, q))
                    res = model.fit()
                    bic = res.bic if hasattr(res, "bic") else np.inf
                    if not np.isnan(bic) and bic < best_bic:
                        best_bic = bic
                        best_order = (p, d, q)
                        best_res = res
                except Exception:
                    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
                    continue
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ (best_res –æ—Å—Ç–∞–µ—Ç—Å—è None) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º (1,1,1)
    if best_res is None:
        best_order = (1, 1, 1)
        best_res = ARIMA(y, order=best_order).fit()
    # –ü—Ä–æ–≥–Ω–æ–∑
    forecast_res = best_res.get_forecast(steps=int(horizon))
    mean_forecast = forecast_res.predicted_mean
    conf_int = forecast_res.conf_int(alpha=0.1)
    fc_df = pd.DataFrame(
        {
            "date": mean_forecast.index.to_timestamp() if hasattr(mean_forecast.index, "to_timestamp") else mean_forecast.index,
            "forecast": mean_forecast.values,
            "lower": conf_int.iloc[:, 0].values,
            "upper": conf_int.iloc[:, 1].values,
        }
    )
    hist_df = y.reset_index().rename(columns={value_col: "value", date_col: "date"})
    return hist_df, fc_df, best_order


def business_summary_for_series(col_name, stats_dict, norm_res, n_outliers):
    mean_ = stats_dict.get("mean")
    median_ = stats_dict.get("median")
    cv = stats_dict.get("cv")
    skew = stats_dict.get("skewness")
    txt = [f"–ü–æ –º–µ—Ç—Ä–∏–∫–µ **{col_name}**:"]

    if not np.isnan(mean_) and not np.isnan(median_):
        txt.append("- —Å—Ä–µ–¥–Ω–µ–µ –∏ –º–µ–¥–∏–∞–Ω–∞ **–±–ª–∏–∑–∫–∏**." if abs(mean_ - median_) / (abs(median_) + 1e-9) <= 0.2
                   else "- —Å—Ä–µ–¥–Ω–µ–µ –∏ –º–µ–¥–∏–∞–Ω–∞ **–∑–∞–º–µ—Ç–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è** ‚Üí –≤–ª–∏—è–µ—Ç —Å–∫–æ—Å/–≤—ã–±—Ä–æ—Å—ã.")
    if not np.isnan(cv):
        txt.append("- –º–µ—Ç—Ä–∏–∫–∞ **–æ—á–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω–∞**." if cv < 0.1 else
                   "- –º–µ—Ç—Ä–∏–∫–∞ **—É–º–µ—Ä–µ–Ω–Ω–æ –∏–∑–º–µ–Ω—á–∏–≤–∞**." if cv < 0.3 else
                   "- **–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å**, –≤–æ–∑–º–æ–∂–Ω—ã —Å–∫–∞—á–∫–∏.")
    if not np.isnan(skew):
        txt.append("- —Ö–≤–æ—Å—Ç –≤–ø—Ä–∞–≤–æ (—Ä–µ–¥–∫–∏–µ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)." if skew > 0.5 else
                   "- —Ö–≤–æ—Å—Ç –≤–ª–µ–≤–æ (–∏–Ω–æ–≥–¥–∞ –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)." if skew < -0.5 else
                   "- –ø–µ—Ä–µ–∫–æ—Å–∞ –ø–æ —Ö–≤–æ—Å—Ç–∞–º –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.")
    if n_outliers > 0:
        txt.append(f"- –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã–±—Ä–æ—Å—ã: ~{n_outliers} –∑–∞–ø–∏—Å–µ–π.")
    if norm_res and "p_value" in norm_res:
        txt.append("- —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ **–Ω–æ—Ä–º–∞–ª—å–Ω–æ** (–ø–æ –®–∞–ø–∏—Ä–æ)." if norm_res["p_value"] >= 0.05
                   else "- —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ **–Ω–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ** ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–¥–∏–∞–Ω—É/IQR.")
    return "\n".join(txt)


def business_summary_for_correlation(col1, col2, r, p):
    if np.isnan(r): return "–°–≤—è–∑—å –Ω–µ –æ—Ü–µ–Ω–µ–Ω–∞ (–º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö)."
    av = abs(r)
    strength = "–ø–æ—á—Ç–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç" if av < 0.1 else "—Å–ª–∞–±–∞—è" if av < 0.3 else "—É–º–µ—Ä–µ–Ω–Ω–∞—è" if av < 0.7 else "—Å–∏–ª—å–Ω–∞—è"
    sign = "–ø—Ä—è–º–∞—è" if r > 0 else "–æ–±—Ä–∞—Ç–Ω–∞—è"
    out = f"–ú–µ–∂–¥—É **{col1}** –∏ **{col2}** ‚Äî **{strength} {sign} —Å–≤—è–∑—å** (r‚âà{r:.2f})."
    out += " –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞." if p < 0.05 else " –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Å–ª–∞–±–∞—è."
    return out


def business_summary_for_ts(global_feats):
    txt = []
    slope = global_feats.get("slope_trend"); change_pct = global_feats.get("change_pct"); cv = global_feats.get("cv")
    if not np.isnan(slope):
        txt.append("- —Ç—Ä–µ–Ω–¥ —Ä–∞—Å—Ç—É—â–∏–π." if slope > 0 else "- —Ç—Ä–µ–Ω–¥ –Ω–∏—Å—Ö–æ–¥—è—â–∏–π." if slope < 0 else "- —Ç—Ä–µ–Ω–¥ –Ω–µ –≤—ã—Ä–∞–∂–µ–Ω.")
    if not np.isnan(change_pct):
        sign = "+" if change_pct >= 0 else ""
        txt.append(f"- –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ç–∞—Ä—Ç–∞ –¥–æ –∫–æ–Ω—Ü–∞: **{sign}{change_pct:.1f}%**.")
    if not np.isnan(cv):
        txt.append("- —Ä—è–¥ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π." if cv < 0.1 else "- —É–º–µ—Ä–µ–Ω–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è." if cv < 0.3 else "- —Ä—è–¥ –≤–æ–ª–∞—Ç–∏–ª–µ–Ω.")
    return "\n".join(txt)


def business_summary_for_ab(group_a, group_b, mean_a, mean_b, diff, p_val, alpha, d_value):
    txt = [
        f"Mean **{group_a}** ‚âà {mean_a:.2f}, Mean **{group_b}** ‚âà {mean_b:.2f}.",
        f"–†–∞–∑–Ω–∏—Ü–∞ (B - A) ‚âà {diff:.2f}."
    ]
    txt.append("–†–∞–∑–ª–∏—á–∏—è **–∑–Ω–∞—á–∏–º—ã**." if p_val < alpha else "–ó–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ—Ç.")
    if not np.isnan(d_value):
        ad = abs(d_value)
        eff = "–æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π" if ad < 0.2 else "–Ω–µ–±–æ–ª—å—à–æ–π" if ad < 0.5 else "—Å—Ä–µ–¥–Ω–∏–π" if ad < 0.8 else "–±–æ–ª—å—à–æ–π"
        txt.append(f"–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (d‚âà{d_value:.2f}) ‚Üí {eff}.")
    return "\n".join(txt)


def build_excel_report(
    df: pd.DataFrame,
    stats_df: pd.DataFrame | None = None,
    corr_matrix: pd.DataFrame | None = None,
    dq: pd.DataFrame | None = None,
    text_blocks: dict | None = None,
) -> BytesIO:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π Excel-–æ—Ç—á—ë—Ç:
    - –ª–∏—Å—Ç Data: —Å–∞–º–∏ –¥–∞–Ω–Ω—ã–µ;
    - –ª–∏—Å—Ç BasicStats: –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞;
    - –ª–∏—Å—Ç Correlations: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏;
    - –ª–∏—Å—Ç DataQuality: –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö;
    - –ª–∏—Å—Ç AI_Summary: —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–≤–æ–¥–∫–∏ (EDA/–±–∏–∑–Ω–µ—Å).
    """
    output = BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        # –¢–∏—Ç—É–ª—å–Ω—ã–π –ª–∏—Å—Ç (Cover)
        try:
            cover_rows: list[dict] = []
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            cover_rows.append({"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–°—Ç—Ä–æ–∫", "–ó–Ω–∞—á–µ–Ω–∏–µ": int(df.shape[0])})
            cover_rows.append({"–ü–∞—Ä–∞–º–µ—Ç—Ä": "–°—Ç–æ–ª–±—Ü–æ–≤", "–ó–Ω–∞—á–µ–Ω–∏–µ": int(df.shape[1])})
            # –†–µ–∑—é–º–µ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (–æ–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏)
            if text_blocks:
                for name, txt in text_blocks.items():
                    if isinstance(txt, str) and txt.strip():
                        cover_rows.append({"–ü–∞—Ä–∞–º–µ—Ç—Ä": f"{name}", "–ó–Ω–∞—á–µ–Ω–∏–µ": txt[:200] + ("‚Ä¶" if len(txt) > 200 else "")})
            cover_df = pd.DataFrame(cover_rows)
            cover_df.to_excel(writer, sheet_name="Cover", index=False)
        except Exception:
            # –¥–∞–∂–µ –µ—Å–ª–∏ Cover —Å–æ–∑–¥–∞—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
            pass
        # –î–∞–Ω–Ω—ã–µ
        try:
            df.to_excel(writer, sheet_name="Data", index=False)
        except Exception:
            # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame
            if isinstance(df, pd.DataFrame):
                df.to_excel(writer, sheet_name="Data", index=False)
        # BasicStats
        if isinstance(stats_df, pd.DataFrame) and not stats_df.empty:
            try:
                stats_df.to_excel(writer, sheet_name="BasicStats")
            except Exception:
                pass
        # Correlations
        if isinstance(corr_matrix, pd.DataFrame) and not corr_matrix.empty:
            try:
                corr_matrix.to_excel(writer, sheet_name="Correlations")
            except Exception:
                pass
        # DataQuality
        if isinstance(dq, pd.DataFrame) and not dq.empty:
            try:
                dq.to_excel(writer, sheet_name="DataQuality", index=False)
            except Exception:
                pass
        # AI_Summary
        if text_blocks:
            try:
                rows = [
                    {"section": k, "text": v}
                    for k, v in text_blocks.items()
                    if isinstance(v, str) and v.strip()
                ]
                if rows:
                    summary_df = pd.DataFrame(rows)
                    summary_df.to_excel(writer, sheet_name="AI_Summary", index=False)
            except Exception:
                pass
    output.seek(0)
    return output

def build_pptx_report(
    df: pd.DataFrame,
    stats_df: pd.DataFrame | None = None,
    corr_matrix: pd.DataFrame | None = None,
    dq: pd.DataFrame | None = None,
    summary_text: str = "",
) -> BytesIO:
    """
    –°–æ–∑–¥–∞—ë—Ç –ø—Ä–æ—Å—Ç–æ–π PPTX-–æ—Ç—á—ë—Ç. –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏—Ç—É–ª—å–Ω—ã–π —Å–ª–∞–π–¥, —Å–ª–∞–π–¥ —Å–æ —Å–≤–æ–¥–∫–æ–π,
    —Ç–∞–±–ª–∏—Ü—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç BytesIO —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Ñ–∞–π–ª–∞. –ï—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ pptx –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª.
    """
    # –ï—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ pptx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –±—É—Ñ–µ—Ä
    if Presentation is None:
        return BytesIO()
    prs = Presentation()
    # –¢–∏—Ç—É–ª—å–Ω—ã–π —Å–ª–∞–π–¥
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    slide.shapes.title.text = "–û—Ç—á—ë—Ç: –ê–≤—Ç–æ-EDA"
    subtitle = slide.placeholders[1]
    subtitle.text = f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {df.shape[0]}, —Å—Ç–æ–ª–±—Ü–æ–≤: {df.shape[1]}"

    # –°–ª–∞–π–¥ —Å–≤–æ–¥–∫–∏
    if summary_text:
        layout = prs.slide_layouts[1]  # Title and Content
        slide = prs.slides.add_slide(layout)
        slide.shapes.title.text = "–°–≤–æ–¥–∫–∞"
        body = slide.shapes.placeholders[1].text_frame
        body.clear()
        for line in summary_text.split("\n"):
            p = body.add_paragraph()
            p.text = line
            p.font.size = Pt(12)

    # –°–ª–∞–π–¥ –±–∞–∑–æ–≤—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
    if isinstance(stats_df, pd.DataFrame) and not stats_df.empty:
        layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(layout)
        slide.shapes.title.text = "–ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫/—Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        table_rows = min(len(stats_df.index) + 1, 15)
        table_cols = min(len(stats_df.columns) + 1, 8)
        display_df = stats_df.copy()
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for c in display_df.columns:
            display_df[c] = display_df[c].apply(
                lambda x: f"{x:.3g}" if isinstance(x, (int, float, np.floating)) and not pd.isna(x) else str(x)
            )
        display_df = display_df.iloc[: table_rows - 1, : table_cols - 1]
        tbl = slide.shapes.add_table(
            rows=table_rows,
            cols=table_cols,
            left=Inches(0.5),
            top=Inches(2),
            width=Inches(9),
            height=Inches(4),
        ).table
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        tbl.cell(0, 0).text = "–ú–µ—Ç—Ä–∏–∫–∞"
        for j, col in enumerate(display_df.columns):
            tbl.cell(0, j + 1).text = str(col)
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏
        for i, idx in enumerate(display_df.index):
            tbl.cell(i + 1, 0).text = str(idx)
            for j, col in enumerate(display_df.columns):
                tbl.cell(i + 1, j + 1).text = str(display_df.loc[idx, col])

    # –°–ª–∞–π–¥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    if isinstance(corr_matrix, pd.DataFrame) and not corr_matrix.empty:
        layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(layout)
        slide.shapes.title.text = "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"
        strong = get_strong_correlations(corr_matrix, threshold=0.5)
        if not strong.empty:
            strong = strong.head(10)[["feature_1", "feature_2", "r"]]
            rows = len(strong.index) + 1
            cols = 3
            tbl = slide.shapes.add_table(
                rows=rows,
                cols=cols,
                left=Inches(0.5),
                top=Inches(2),
                width=Inches(9),
                height=Inches(4),
            ).table
            tbl.cell(0, 0).text = "X"
            tbl.cell(0, 1).text = "Y"
            tbl.cell(0, 2).text = "r"
            for i, (_, row) in enumerate(strong.iterrows()):
                tbl.cell(i + 1, 0).text = str(row["feature_1"])
                tbl.cell(i + 1, 1).text = str(row["feature_2"])
                tbl.cell(i + 1, 2).text = f"{row['r']:.2f}"
        else:
            body = slide.shapes.placeholders[1].text_frame
            body.clear()
            p = body.add_paragraph()
            p.text = "–°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã."
            p.font.size = Pt(12)

    # –°–ª–∞–π–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    if isinstance(dq, pd.DataFrame) and not dq.empty:
        layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(layout)
        slide.shapes.title.text = "–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö"
        body = slide.shapes.placeholders[1].text_frame
        body.clear()
        hi = dq[dq["missing_%"] > 30]
        if not hi.empty:
            p = body.add_paragraph()
            p.text = "–ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ >30%:"
            p.font.bold = True
            for _, row in hi.iterrows():
                p = body.add_paragraph()
                p.text = f"‚Ä¢ {row['column']} ({row['missing_%']:.1f}%)"
        else:
            p = body.add_paragraph()
            p.text = "–ü—Ä–æ–ø—É—Å–∫–æ–≤ >30% –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
        suggestions = dq[(dq["type_suggestion"].notna()) | (dq["mixed_types"] == True)]
        if not suggestions.empty:
            p = body.add_paragraph()
            p.text = "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º:"
            p.font.bold = True
            for _, row in suggestions.iterrows():
                if row["type_suggestion"]:
                    p = body.add_paragraph()
                    p.text = f"‚Ä¢ {row['column']}: –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ {row['type_suggestion']} (—Ç–µ–∫—É—â–∏–π —Ç–∏–ø {row['dtype']})"
                if row["mixed_types"]:
                    p = body.add_paragraph()
                    p.text = f"‚Ä¢ {row['column']}: —Å–º–µ—à–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –∑–Ω–∞—á–µ–Ω–∏–π, –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç"
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    buffer = BytesIO()
    prs.save(buffer)
    buffer.seek(0)
    return buffer
def generate_ai_text_with_fallback(base_text: str, extra_prompt: str, api_key: str | None) -> str:
    """
    –ü—Ä–æ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É —Å –ø–æ–º–æ—â—å—é –≤–Ω–µ—à–Ω–µ–π LLM (OpenAI).
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ —Å –Ω–æ–≤–æ–π (1.x), –∏ —Å–æ —Å—Ç–∞—Ä–æ–π (0.x) –≤–µ—Ä—Å–∏–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ openai.
    –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –∑–∞–¥–∞–Ω –∏–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    if not api_key:
        # AI-—Ä–µ–∂–∏–º –Ω–µ –≤–∫–ª—é—á—ë–Ω ‚Äî —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫–∞—Ö
        return base_text

    try:
        import openai  # type: ignore

        prompt = extra_prompt + "\n\n" + base_text

        # –ù–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç (openai>=1.x)
        if hasattr(openai, "OpenAI"):
            client = openai.OpenAI(api_key=api_key)
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=600,
            )
            content = resp.choices[0].message.content

        # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç (openai 0.x)
        else:
            openai.api_key = api_key
            resp = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=600,
            )
            content = resp["choices"][0]["message"]["content"]

        return content.strip() if content else base_text

    except Exception:
        # –ù–∏—á–µ–≥–æ –Ω–µ –ª–æ–º–∞–µ–º, –ø—Ä–æ—Å—Ç–æ —Ç–∏—Ö–æ –æ—Å—Ç–∞—ë–º—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–≤–æ–¥–∫–µ
        return base_text

def get_ai_config():
    mode = st.session_state.get("ai_mode", "local")
    api_key = st.session_state.get("ai_api_key") if mode == "openai" else None
    return mode, api_key



def render_ai_block(local_text: str, button_label: str, cache_key: str, extra_prompt: str = ""):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–ª–æ–∫: –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ + –∫–Ω–æ–ø–∫–∞ –≤—ã–∑–æ–≤–∞ AI + –≤—ã–≤–æ–¥ AI-—Ä–µ–∑—é–º–µ.
    cache_key ‚Äì —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è session_state (–Ω–∞–ø—Ä–∏–º–µ—Ä 'ai_dq', 'ai_ts', 'ai_ab_main').
    """
    if not local_text:
        return

    st.markdown("üîé **–õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ:**")
    st.markdown(f'<div class="business-summary">{local_text}</div>', unsafe_allow_html=True)

    mode, api_key = get_ai_config()
    if mode != "openai" or not api_key:
        st.caption("AI-—Ä–µ–∑—é–º–µ –¥–æ—Å—Ç—É–ø–Ω–æ –≤ —Ä–µ–∂–∏–º–µ OpenAI –Ω–∞ —à–∞–≥–µ 3.")
        return

    if "ai_summaries" not in st.session_state:
        st.session_state["ai_summaries"] = {}

    if st.button(button_label, key=f"btn_{cache_key}"):
        ai_text = generate_ai_text_with_fallback(local_text, extra_prompt, api_key)
        st.session_state["ai_summaries"][cache_key] = ai_text

    if cache_key in st.session_state["ai_summaries"]:
        st.markdown("ü§ñ **AI-—Ä–µ–∑—é–º–µ:**")
        st.markdown(
            f'<div class="business-summary">{st.session_state["ai_summaries"][cache_key]}</div>',
            unsafe_allow_html=True,
        )


def ai_enrich_text(base_text: str, extra_prompt: str = "") -> str:
    """
    –£–°–¢–ê–†–ï–í–®–ê–Ø –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è AI: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    –ë–æ–ª—å—à–µ –ù–ï –≤—ã–ø–æ–ª–Ω—è–µ—Ç –Ω–∏–∫–∞–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –≤–Ω–µ—à–Ω–∏–º –º–æ–¥–µ–ª—è–º –∏ –≤—Å–µ–≥–¥–∞
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.

    –í—Å—è —Ä–∞–±–æ—Ç–∞ —Å AI —Ç–µ–ø–µ—Ä—å –∏–¥—ë—Ç —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é render_ai_block, –∫–æ—Ç–æ—Ä–∞—è
    –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞–∂–∞—Ç–∏—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–Ω–æ–ø–∫–∏ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ.
    """
    return base_text


def auto_eda_summary(df: pd.DataFrame, stats_df: pd.DataFrame, corr: pd.DataFrame | None, dq: pd.DataFrame, cols: list[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –∞–≤—Ç–æ-—Å–≤–æ–¥–∫—É –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º.
    –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫–∞—Ö, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π."""
    txt: list[str] = []
    if not cols:
        return "–ù–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è EDA."

    # 1) –°–∞–º—ã–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∏ —Å–∞–º—ã–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ CV
    if "cv" in stats_df.columns:
        cv_sorted = stats_df["cv"].abs().sort_values().dropna()
        if not cv_sorted.empty:
            low_name = cv_sorted.index[0]
            low_val = cv_sorted.iloc[0]
            high_name = cv_sorted.index[-1]
            high_val = cv_sorted.iloc[-1]
            txt.append(f"- –°–∞–º–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: **{low_name}** (CV‚âà{low_val:.2f}).")
            if len(cv_sorted) > 1:
                txt.append(f"- –ù–∞–∏–±–æ–ª–µ–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: **{high_name}** (CV‚âà{high_val:.2f}).")

    # 2) –ú–µ—Ç—Ä–∏–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    if "missing_%" in dq.columns:
        hi_missing = dq.sort_values("missing_%", ascending=False).head(3)
        serious = hi_missing[hi_missing["missing_%"] > 20]
        if not serious.empty:
            probs = ", ".join(f"{idx} ({row['missing_%']:.1f}%)" for idx, row in serious.iterrows())
            txt.append(f"- –ö–æ–ª–æ–Ω–∫–∏ —Å –∑–∞–º–µ—Ç–Ω–æ–π –¥–æ–ª–µ–π –ø—Ä–æ–ø—É—Å–∫–æ–≤: {probs}.")
        elif (dq["missing_%"] > 0).any():
            txt.append("- –ü—Ä–æ–ø—É—Å–∫–∏ –µ—Å—Ç—å, –Ω–æ –∏—Ö –¥–æ–ª—è –≤–æ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö < 20%.")
        else:
            txt.append("- –ü—Ä–æ–ø—É—Å–∫–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–æ –≤—Å–µ—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö.")

    # 3) –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏
    if corr is not None and not corr.empty:
        cm = corr.copy()
        for c in cm.columns:
            cm.loc[c, c] = 0.0
        strong_pairs = []
        for i in cm.columns:
            for j in cm.columns:
                if j <= i:
                    continue
                r = cm.loc[i, j]
                if abs(r) >= 0.5:
                    strong_pairs.append((i, j, r))
        if strong_pairs:
            strong_pairs.sort(key=lambda x: -abs(x[2]))
            top_desc = ", ".join(f"{a}‚Äì{b} (r‚âà{r:.2f})" for a, b, r in strong_pairs[:3])
            txt.append(f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏: {top_desc}.")
        else:
            txt.append("- –°–∏–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.")

    # 4) –°–∫–æ—Å –∏ —Ç—è–∂—ë–ª—ã–µ —Ö–≤–æ—Å—Ç—ã
    skew_cols = []
    heavy_tail = []
    if "skewness" in stats_df.columns:
        for idx, row in stats_df.iterrows():
            skew = row.get("skewness")
            if isinstance(skew, (int, float)):
                if skew > 1:
                    skew_cols.append(f"{idx} (–≤–ø—Ä–∞–≤–æ)")
                elif skew < -1:
                    skew_cols.append(f"{idx} (–≤–ª–µ–≤–æ)")
    if "kurtosis" in stats_df.columns:
        for idx, row in stats_df.iterrows():
            kurt = row.get("kurtosis")
            if isinstance(kurt, (int, float)) and kurt > 4:
                heavy_tail.append(f"{idx} (kurt‚âà{kurt:.1f})")
    if skew_cols:
        txt.append("- –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —Å–∫–æ—Å–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: " + ", ".join(skew_cols) + ".")
    if heavy_tail:
        txt.append("- –ú–µ—Ç—Ä–∏–∫–∏ —Å —Ç—è–∂—ë–ª—ã–º–∏ —Ö–≤–æ—Å—Ç–∞–º–∏ (–º–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π): " + ", ".join(heavy_tail) + ".")

    # 5) –î–∏–∞–ø–∞–∑–æ–Ω—ã –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
    ranges = []
    for c in cols:
        if c in stats_df.index:
            row = stats_df.loc[c]
            min_v = row.get("min", float("nan"))
            max_v = row.get("max", float("nan"))
            if not (math.isnan(min_v) or math.isnan(max_v)):
                ranges.append(f"{c}: [{min_v:.3g}; {max_v:.3g}]")
    if ranges:
        txt.append("- –î–∏–∞–ø–∞–∑–æ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º: " + "; ".join(ranges) + ".")

    if not txt:
        txt.append("–ü–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É.")
    return "\n".join(txt)


def build_auto_eda_html(df: pd.DataFrame, cols: list[str], stats_df: pd.DataFrame, corr: pd.DataFrame | None,
                        dq: pd.DataFrame, summary_text: str) -> bytes:
    """–°—Ç—Ä–æ–∏—Ç HTML-–æ—Ç—á—ë—Ç –ø–æ –∞–≤—Ç–æ-EDA —Å –ø—Ä–æ—Å—Ç—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º."""
    style = """
    <style>
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 20px; background-color: #f7f7fb; }
    h1 { color: #111827; }
    h2 { color: #1f2937; border-bottom: 1px solid #e5e7eb; padding-bottom: 4px; }
    .summary-box { background-color: #eef2ff; border-radius: 8px; padding: 12px 16px; margin-bottom: 16px; }
    .summary-box pre { white-space: pre-wrap; font-family: inherit; margin: 0; }
    table { border-collapse: collapse; margin-top: 8px; }
    th, td { padding: 4px 8px; border: 1px solid #e5e7eb; font-size: 12px; }
    th { background-color: #f3f4f6; }
    </style>
    """
    html_parts: list[str] = []
    html_parts.append("<html><head><meta charset='utf-8'><title>Auto EDA Report</title>" + style + "</head><body>")
    html_parts.append("<h1>Auto EDA –æ—Ç—á—ë—Ç</h1>")
    html_parts.append("<div class='summary-box'><h2>–°–≤–æ–¥–∫–∞</h2>")
    html_parts.append("<pre>" + summary_text.replace("<", "&lt;").replace(">", "&gt;") + "</pre></div>")

    html_parts.append("<h2>–ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏</h2>")
    html_parts.append(stats_df.to_html(border=0, classes="table-stats", float_format=lambda x: f"{x:.4g}"))

    if corr is not None and not corr.empty:
        html_parts.append("<h2>–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏</h2>")
        html_parts.append(corr.to_html(border=0, classes="table-corr", float_format=lambda x: f"{x:.4g}"))

    html_parts.append("<h2>–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö</h2>")
    html_parts.append(dq.to_html(border=0, classes="table-dq", float_format=lambda x: f"{x:.4g}"))

    html_parts.append("</body></html>")
    return "\n".join(html_parts).encode("utf-8")

def make_stl_figure(comp_df: pd.DataFrame):
    fig = make_subplots(rows=4, cols=1, shared_xaxes=True, vertical_spacing=0.02)
    fig.add_trace(go.Scatter(x=comp_df["date"], y=comp_df["observed"], name="–ù–∞–±–ª—é–¥–µ–Ω–∏—è"), row=1, col=1)
    fig.add_trace(go.Scatter(x=comp_df["date"], y=comp_df["trend"], name="–¢—Ä–µ–Ω–¥"), row=2, col=1)
    fig.add_trace(go.Scatter(x=comp_df["date"], y=comp_df["seasonal"], name="–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å"), row=3, col=1)
    fig.add_trace(go.Scatter(x=comp_df["date"], y=comp_df["resid"], name="–û—Å—Ç–∞—Ç–æ–∫"), row=4, col=1)
    fig.update_layout(height=650, title="STL-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ —Ä—è–¥–∞", hovermode="x unified",
                      margin=dict(t=40, r=20, b=40, l=40))
    return fig


# ------------------------- DEMO DATA -------------------------
def get_demo_dataset(name: str) -> pd.DataFrame:
    """–ü—Ä–æ—Å—Ç—ã–µ –¥–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç—ã, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –ø–æ–∏–≥—Ä–∞—Ç—å—Å—è –±–µ–∑ —Å–≤–æ–∏—Ö —Ñ–∞–π–ª–æ–≤."""
    np.random.seed(42)
    if name == "–ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º (TS)":
        dates = pd.date_range(end=pd.Timestamp.today().normalize(), periods=120, freq="D")
        base = np.linspace(100, 200, len(dates))
        season = 20 * np.sin(np.linspace(0, 4 * np.pi, len(dates)))
        noise = np.random.normal(0, 10, len(dates))
        sales = base + season + noise
        df = pd.DataFrame({
            "date": dates,
            "sales": sales,
            "channel": np.random.choice(["online", "offline"], size=len(dates)),
        })
        return df
    else:
        # –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –∫–∞–º–ø–∞–Ω–∏—è: A/B, CTR, –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
        n = 1000
        df = pd.DataFrame({
            "user_id": np.arange(1, n+1),
            "group": np.random.choice(["A", "B"], size=n),
            "impressions": np.random.randint(10, 200, size=n),
        })
        # –ü—Ä–∏–¥—É–º–∞–µ–º –∫–ª–∏–∫–∏ –∏ –∑–∞–∫–∞–∑—ã —Å –Ω–µ–±–æ–ª—å—à–∏–º uplift –≤ –≥—Ä—É–ø–ø–µ B
        p_click_A, p_click_B = 0.08, 0.11
        p_conv_A, p_conv_B = 0.02, 0.03
        mask_A = df["group"] == "A"
        clicks = np.zeros(n, dtype=int)
        orders = np.zeros(n, dtype=int)
        for i in range(n):
            if mask_A.iloc[i]:
                clicks[i] = np.random.binomial(df["impressions"].iloc[i], p_click_A)
                orders[i] = np.random.binomial(clicks[i], p_conv_A)
            else:
                clicks[i] = np.random.binomial(df["impressions"].iloc[i], p_click_B)
                orders[i] = np.random.binomial(clicks[i], p_conv_B)
        df["clicks"] = clicks
        df["orders"] = orders
        df["revenue"] = df["orders"] * np.random.uniform(50, 150, size=n)
        return df


# ------------------------- APP -------------------------
def main():
    st.set_page_config(page_title="–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è v5.6", layout="wide")

    if "wizard_step" not in st.session_state:
        st.session_state["wizard_step"] = 1

    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AI-–ø–æ–º–æ—â–Ω–∏–∫–∞
    if "ai_mode" not in st.session_state:
        st.session_state["ai_mode"] = "local"
    if "ai_api_key" not in st.session_state:
        st.session_state["ai_api_key"] = ""
    # –§–ª–∞–≥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å–≤–æ–¥–æ–∫: –≤—ã–∫–ª—é—á–µ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if "auto_ai_call" not in st.session_state:
        st.session_state["auto_ai_call"] = False

    st.title("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è v5.6 (EDA + –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ)")

    with st.expander("‚ÑπÔ∏è –ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º", expanded=True):
        st.markdown(
            "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ **–º–∞—Å—Ç–µ—Ä–∞ –∏–∑ 4 —à–∞–≥–æ–≤**:\n"
            "1) **–®–∞–≥ 1. –î–∞–Ω–Ω—ã–µ** ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç–µ CSV/Excel –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –¥–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç.\n"
            "2) **–®–∞–≥ 2. –§–∏–ª—å—Ç—Ä—ã –∏ –∫–æ–Ω—Ñ–∏–≥** ‚Äî –∑–∞–¥–∞–π—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã, –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n"
            "3) **–®–∞–≥ 3. AI-–ø–æ–º–æ—â–Ω–∏–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)** ‚Äî –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º AI –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–≤–µ–¥–∏—Ç–µ API-–∫–ª—é—á.\n"
            "4) **–®–∞–≥ 4. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á—ë—Ç—ã** ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∫–ª–∞–¥–∫–∏: –¥–∞–Ω–Ω—ã–µ, –±–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã, –≤—ã–±—Ä–æ—Å—ã, —Ñ–∏—á–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, A/B, —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏ –æ—Ç—á—ë—Ç—ã."
        )

    # ------------------------- STEP 1: DATA SOURCE -------------------------
    st.sidebar.header("–®–∞–≥ 1. –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö")

    source_mode = st.sidebar.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:",
        ["–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª", "–î–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç"],
        key="data_source_mode",
    )

    main_file = merge_file = None
    df_raw = df_merge = None

    if source_mode == "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª":
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: CSV, Excel, Parquet, Feather
        main_file = st.sidebar.file_uploader(
            "–û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª (CSV/XLSX/Parquet/Feather)",
            type=["csv", "xlsx", "xls", "parquet", "feather"],
        )
        merge_file = st.sidebar.file_uploader(
            "–í—Ç–æ—Ä–æ–π —Ñ–∞–π–ª –¥–ª—è merge (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
            type=["csv", "xlsx", "xls", "parquet", "feather"],
        )

        def read_user_file(file):
            """
            –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π
            –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ñ–æ—Ä–º–∞—Ç–∞ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤.

            –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ—Å—Ç–æ–π –∫–µ—à –ø–æ –∏–º–µ–Ω–∏ –∏ —Ä–∞–∑–º–µ—Ä—É —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞—Ç—å
            –∏ –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—å —Ç–∏–ø—ã –Ω–∞ –∫–∞–∂–¥–æ–º –ø–µ—Ä–µ—Ä–µ–Ω–¥–µ—Ä–µ Streamlit.
            """
            # –ü—Ä–æ—Å—Ç–æ–π –∫–µ—à –≤ session_state: –∫–ª—é—á ‚Äî (name, size)
            cache = st.session_state.setdefault("file_cache", {})
            try:
                file_id = (getattr(file, "name", None), getattr(file, "size", None))
            except Exception:
                file_id = (getattr(file, "name", None), None)
            if file_id in cache:
                return cache[file_id]

            name = file.name.lower()
            df_tmp: pd.DataFrame
            # CSV
            if name.endswith(".csv"):
                # –ü—Ä–æ–±—É–µ–º –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–±–æ—Ä: –µ—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–±–æ—Ä –Ω–µ —É–¥–∞—ë—Ç—Å—è,
                # –ø–æ–≤—Ç–æ—Ä–∏–º —Å python-engine, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å.
                try:
                    df_tmp = pd.read_csv(file)
                except Exception:
                    try:
                        df_tmp = pd.read_csv(file, engine="python")
                    except Exception as ee:
                        raise ee
            # Parquet
            elif name.endswith(".parquet"):
                df_tmp = pd.read_parquet(file)
            # Feather
            elif name.endswith(".feather"):
                df_tmp = pd.read_feather(file)
            # Excel
            else:
                df_tmp = pd.read_excel(file)
            # –ê–≤—Ç–æ-–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
            try:
                df_tmp = auto_coerce_dataframe_types(df_tmp)
            except Exception:
                pass

            cache[file_id] = df_tmp
            st.session_state["file_cache"] = cache
            return df_tmp

        if main_file is not None:
            try:
                df_raw = read_user_file(main_file)
            except Exception as e:
                st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")

        if merge_file is not None:
            try:
                df_merge = read_user_file(merge_file)
            except Exception as e:
                st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
    else:
        st.sidebar.markdown("**–î–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç** ‚Äî –º–æ–∂–Ω–æ –±—ã—Å—Ç—Ä–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏.")
        demo_name = st.sidebar.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–º–æ-–Ω–∞–±–æ—Ä",
            ["–ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º (TS)", "–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∞—è –∫–∞–º–ø–∞–Ω–∏—è (A/B)"],
            key="demo_name_select",
        )
        df_raw = get_demo_dataset(demo_name)
        st.sidebar.success(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–º–æ-–Ω–∞–±–æ—Ä: {demo_name}")

    has_data = df_raw is not None

    # ------------------------- WIZARD STEPPER -------------------------
    step = st.session_state.get("wizard_step", 1)
    step = max(1, min(4, int(step)))
    st.session_state["wizard_step"] = step

    st.markdown("### üö¶ –®–∞–≥–∏ –∞–Ω–∞–ª–∏–∑–∞")
    cols = st.columns(4)
    labels = [
        "1. –î–∞–Ω–Ω—ã–µ",
        "2. –§–∏–ª—å—Ç—Ä—ã –∏ –∫–æ–Ω—Ñ–∏–≥",
        "3. AI-–ø–æ–º–æ—â–Ω–∏–∫",
        "4. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á—ë—Ç—ã",
    ]
    for i, col in enumerate(cols, start=1):
        with col:
            if i < step:
                cls = "step-badge step-done"
                prefix = "‚úÖ"
            elif i == step:
                cls = "step-badge step-active"
                prefix = "üü¢"
            else:
                cls = "step-badge step-future"
                prefix = "‚ö™"
            st.markdown(f'<span class="{cls}">{prefix} {labels[i-1]}</span>', unsafe_allow_html=True)

    c_prev, c_next = st.columns([1, 1])
    with c_prev:
        if st.button("‚¨ÖÔ∏è –ù–∞–∑–∞–¥", disabled=(step <= 1)):
            st.session_state["wizard_step"] = max(1, step - 1)
            # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Streamlit
            if hasattr(st, "rerun"):
                st.rerun()
            else:
                st.experimental_rerun()
    with c_next:
        if st.button("–í–ø–µ—Ä—ë–¥ ‚û°Ô∏è", disabled=(step >= 4 or not has_data)):
            st.session_state["wizard_step"] = min(4, step + 1)
            # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Streamlit
            if hasattr(st, "rerun"):
                st.rerun()
            else:
                st.experimental_rerun()

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –¥–∞–ª—å—à–µ —Å–º—ã—Å–ª–∞ –Ω–µ—Ç –∏–¥—Ç–∏
    if not has_data:
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –¥–µ–º–æ-–¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ **—à–∞–≥–µ 1** –≤ —Å–∞–π–¥–±–∞—Ä–µ, –∑–∞—Ç–µ–º –Ω–∞–∂–º–∏—Ç–µ ¬´–í–ø–µ—Ä—ë–¥ ‚û°Ô∏è¬ª.")
        return

    # –ï—Å–ª–∏ –º—ã –µ—â—ë –Ω–∞ —à–∞–≥–µ 1, –Ω–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∏ –ø—Ä–æ—Å–∏–º
    if step == 1:
        st.success(
            f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: **{df_raw.shape[0]}** —Å—Ç—Ä–æ–∫, **{df_raw.shape[1]}** –∫–æ–ª–æ–Ω–æ–∫.\n"
            "–ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ **–®–∞–≥ 2 (–∫–Ω–æ–ø–∫–∞ ¬´–í–ø–µ—Ä—ë–¥ ‚û°Ô∏è¬ª)**, —á—Ç–æ–±—ã –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã, merge –∏ –æ—Ç–∫—Ä—ã—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –≤–∫–ª–∞–¥–∫–∏."
        )
        with st.expander("üìÅ –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏", expanded=True):
            st.dataframe(df_raw.head(20))

        if df_merge is not None:
            with st.expander("üìÅ –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç ‚Äî –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏", expanded=True):
                c_m1, c_m2 = st.columns(2)
                c_m1.metric("–°—Ç—Ä–æ–∫ (2-–π —Ñ–∞–π–ª)", df_merge.shape[0])
                c_m2.metric("–°—Ç–æ–ª–±—Ü–æ–≤ (2-–π —Ñ–∞–π–ª)", df_merge.shape[1])
                st.dataframe(df_merge.head(20))

        return

    # ------------------------- –®–∞–≥ 2 –∏ –≤—ã—à–µ: –ö–û–ù–§–ò–ì + –§–ò–õ–¨–¢–†–´ + –ê–ù–ê–õ–ò–¢–ò–ö–ê -------------------------

    # ------------------------- CONFIG + FILTERS -------------------------
    st.sidebar.header("–®–∞–≥ 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
    config_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (JSON)", type="json")
    if config_file:
        try:
            loaded = json.load(config_file)
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ñ–∏–ª—å—Ç—Ä–æ–≤
            st.session_state["filter_cols"] = loaded.get("filters", {}).get("filter_cols", [])
            for col, spec in loaded.get("filters", {}).get("per_column", {}).items():
                if spec.get("type") == "numeric":
                    st.session_state[f"filter_mode_{col}"] = spec.get("mode", "–î–∏–∞–ø–∞–∑–æ–Ω")
                    if "range" in spec:
                        st.session_state[f"filter_range_{col}"] = tuple(spec["range"])
                    if "values" in spec:
                        st.session_state[f"filter_vals_num_{col}"] = spec["values"]
                else:
                    st.session_state[f"search_{col}"] = spec.get("search", "")
                    st.session_state[f"filter_vals_{col}"] = spec.get("values", [])
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞, –µ—Å–ª–∏ –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
            if "selected_report_columns" in loaded:
                st.session_state["report_num_cols"] = loaded["selected_report_columns"]
            st.sidebar.success("–ö–æ–Ω—Ñ–∏–≥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        except Exception as e:
            st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞: {e}")

    # ------------------------- MERGE + –§–ò–õ–¨–¢–†–´ (–®–∞–≥ 2 –∏ 3) -------------------------
    # –ù–∞ —à–∞–≥–µ 2 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∏ merge.
    # –ù–∞ —à–∞–≥–µ 3 –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ UI.

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    default_filter_cfg = {"filter_cols": [], "per_column": {}}
    default_filter2_cfg = {"filter_cols": [], "per_column": {}}

    if step == 2:
        # 1) –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        st.subheader("üîç –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        df_main = df_raw.copy()
        active_filters_main = []

        filter_cols = st.multiselect("–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç)", df_raw.columns.tolist(),
                                     key="filter_cols")
        filter_config = {"filter_cols": filter_cols, "per_column": {}}

        # 2) –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
        df_second = df_merge.copy() if df_merge is not None else None
        active_filters_second = []

        if df_merge is not None:
            st.subheader("üîç –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            filter2_cols = st.multiselect("–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–≤—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç)", df_merge.columns.tolist(),
                                          key="filter2_cols")
        else:
            filter2_cols = []

        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–±—Ä–æ—Å —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ merge
        if st.button("üîÅ –°–±—Ä–æ—Å–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏ merge"):
            for k in list(st.session_state.keys()):
                if (k.startswith("filter_") or k.startswith("search_") or
                    k.startswith("filter2_") or k.startswith("search2_") or
                    k.startswith("merge_") or k in ("use_merge", "merged_df_cache",
                                                    "df_analysis", "active_filters_all",
                                                    "analysis_cols_default", "filter_config", "filter_config_second")):
                    del st.session_state[k]
            if hasattr(st, "rerun"):
                st.rerun()
            else:
                st.experimental_rerun()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        for col in filter_cols:
            s = df_raw[col]
            if np.issubdtype(s.dtype, np.number):
                mode = st.radio(f"–¢–∏–ø —Ñ–∏–ª—å—Ç—Ä–∞ –¥–ª—è {col} (–æ—Å–Ω–æ–≤–Ω–æ–π)", ["–î–∏–∞–ø–∞–∑–æ–Ω", "–ü–æ –∑–Ω–∞—á–µ–Ω–∏—è–º"],
                                key=f"filter_mode_{col}", horizontal=True)
                cfg = {"type": "numeric", "mode": mode}
                if mode == "–î–∏–∞–ø–∞–∑–æ–Ω":
                    min_v, max_v = float(s.min()), float(s.max())
                    if not (np.isnan(min_v) or np.isnan(max_v)):
                        r = st.slider(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è {col} (–æ—Å–Ω–æ–≤–Ω–æ–π)", min_value=min_v, max_value=max_v,
                                      value=st.session_state.get(f"filter_range_{col}", (min_v, max_v)),
                                      key=f"filter_range_{col}")
                        df_main = df_main[(df_main[col] >= r[0]) & (df_main[col] <= r[1])]
                        active_filters_main.append(f"{col} ‚àà [{r[0]:.3g}; {r[1]:.3g}]")
                        cfg["range"] = [r[0], r[1]]
                else:
                    vals = sorted(s.dropna().unique())
                    default = st.session_state.get(f"filter_vals_num_{col}", vals[:1] if vals else [])
                    chosen = st.multiselect(f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {col} (–æ—Å–Ω–æ–≤–Ω–æ–π)", options=vals, default=default,
                                            key=f"filter_vals_num_{col}")
                    if chosen:
                        df_main = df_main[df_main[col].isin(chosen)]
                        active_filters_main.append(f"{col} ‚àà {{{', '.join(map(str, chosen))}}}")
                        cfg["values"] = list(map(lambda x: x if isinstance(x, (int, float)) else str(x), chosen))
                filter_config["per_column"][col] = cfg
            else:
                uniq = sorted(s.dropna().astype(str).unique())
                search = st.text_input(f"–ü–æ–∏—Å–∫ –ø–æ {col} (–æ—Å–Ω–æ–≤–Ω–æ–π)", value=st.session_state.get(f"search_{col}", ""),
                                       key=f"search_{col}")
                choices = [v for v in uniq if search.lower() in v.lower()] if search else uniq
                default = st.session_state.get(f"filter_vals_{col}", choices[:1] if choices else [])
                chosen = st.multiselect(f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {col} (–æ—Å–Ω–æ–≤–Ω–æ–π)", options=choices, default=default,
                                        key=f"filter_vals_{col}")
                if chosen:
                    df_main = df_main[df_main[col].astype(str).isin(chosen)]
                    active_filters_main.append(f"{col} ‚àà {{{', '.join(chosen)}}}")
                filter_config["per_column"][col] = {"type": "categorical", "search": search, "values": chosen}

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∫–æ –≤—Ç–æ—Ä–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        filter_config_second = {"filter_cols": [], "per_column": {}}
        if df_second is not None and filter2_cols:
            filter_config_second["filter_cols"] = filter2_cols
            for col in filter2_cols:
                s2 = df_merge[col]
                if np.issubdtype(s2.dtype, np.number):
                    mode2 = st.radio(f"–¢–∏–ø —Ñ–∏–ª—å—Ç—Ä–∞ –¥–ª—è {col} (2-–π –¥–∞—Ç–∞—Å–µ—Ç)", ["–î–∏–∞–ø–∞–∑–æ–Ω", "–ü–æ –∑–Ω–∞—á–µ–Ω–∏—è–º"],
                                     key=f"filter2_mode_{col}", horizontal=True)
                    cfg2 = {"type": "numeric", "mode": mode2}
                    if mode2 == "–î–∏–∞–ø–∞–∑–æ–Ω":
                        min2, max2 = float(s2.min()), float(s2.max())
                        if not (np.isnan(min2) or np.isnan(max2)):
                            r2 = st.slider(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è {col} (2-–π –¥–∞—Ç–∞—Å–µ—Ç)", min_value=min2, max_value=max2,
                                           value=st.session_state.get(f"filter2_range_{col}", (min2, max2)),
                                           key=f"filter2_range_{col}")
                            df_second = df_second[(df_second[col] >= r2[0]) & (df_second[col] <= r2[1])]
                            active_filters_second.append(f"[2] {col} ‚àà [{r2[0]:.3g}; {r2[1]:.3g}]")
                            cfg2["range"] = [r2[0], r2[1]]
                    else:
                        vals2 = sorted(s2.dropna().unique())
                        default2 = st.session_state.get(f"filter2_vals_num_{col}", vals2[:1] if vals2 else [])
                        chosen2 = st.multiselect(f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {col} (2-–π –¥–∞—Ç–∞—Å–µ—Ç)", options=vals2, default=default2,
                                                 key=f"filter2_vals_num_{col}")
                        if chosen2:
                            df_second = df_second[df_second[col].isin(chosen2)]
                            active_filters_second.append(f"[2] {col} ‚àà {{{', '.join(map(str, chosen2))}}}")
                            cfg2["values"] = list(map(lambda x: x if isinstance(x, (int, float)) else str(x), chosen2))
                    filter_config_second["per_column"][col] = cfg2
                else:
                    uniq2 = sorted(s2.dropna().astype(str).unique())
                    search2 = st.text_input(f"–ü–æ–∏—Å–∫ –ø–æ {col} (2-–π –¥–∞—Ç–∞—Å–µ—Ç)",
                                            value=st.session_state.get(f"search2_{col}", ""),
                                            key=f"search2_{col}")
                    choices2 = [v for v in uniq2 if search2.lower() in v.lower()] if search2 else uniq2
                    default2 = st.session_state.get(f"filter2_vals_{col}", choices2[:1] if choices2 else [])
                    chosen2 = st.multiselect(f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {col} (2-–π –¥–∞—Ç–∞—Å–µ—Ç)", options=choices2, default=default2,
                                             key=f"filter2_vals_{col}")
                    if chosen2:
                        df_second = df_second[df_second[col].astype(str).isin(chosen2)]
                        active_filters_second.append(f"[2] {col} ‚àà {{{', '.join(chosen2)}}}")
                    filter_config_second["per_column"][col] = {"type": "categorical", "search": search2, "values": chosen2}

        # 3) Merge –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –∫–Ω–æ–ø–∫–æ–π –∏ join –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–æ–ª–æ–Ω–∫–∞–º
        st.subheader("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)")
        df = df_main.copy()
        merged_df = None

        if df_second is not None:
            with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ merge", expanded=False):
                st.caption("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º. "
                           "–ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –∫–∞–∫ –∫–ª—é—á–∏.")
                common_cols = sorted(set(df_main.columns).intersection(df_second.columns))
                if common_cols:
                    join_keys = st.multiselect("–ö–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–æ–±—â–∏–µ –¥–ª—è –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)",
                                               options=common_cols,
                                               default=st.session_state.get("merge_join_keys", []),
                                               key="merge_join_keys")
                else:
                    join_keys = []
                    st.warning("–ù–µ—Ç –æ–±—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è merge.")

                how_join = st.selectbox("–¢–∏–ø join", ["inner", "left", "right", "outer"], key="merge_how")
                merge_btn = st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å merge –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", key="merge_do")
                use_merge = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç merge –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                                        value=st.session_state.get("use_merge", False), key="use_merge")

                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ merge —Ç–æ–ª—å–∫–æ –ø–æ –∫–Ω–æ–ø–∫–µ
                if merge_btn:
                    if not join_keys:
                        st.error("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∫–ª—é—á–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É –¥–ª—è merge.")
                    else:
                        try:
                            with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ–º merge –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤..."):
                                merged_tmp = df_main.merge(df_second, on=join_keys, how=how_join)
                            st.session_state["merged_df_cache"] = merged_tmp
                            st.success(f"–£—Å–ø–µ—à–Ω—ã–π merge: {merged_tmp.shape[0]} —Å—Ç—Ä–æ–∫, {merged_tmp.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤.")
                            st.dataframe(merged_tmp.head(20))
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ merge: {e}")

                # –ï—Å–ª–∏ –≤ —Å–µ—Å—Å–∏–∏ —É–∂–µ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç merge ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                if "merged_df_cache" in st.session_state:
                    merged_cached = st.session_state["merged_df_cache"]
                    st.caption(f"–¢–µ–∫—É—â–∏–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {merged_cached.shape[0]} —Å—Ç—Ä–æ–∫, {merged_cached.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤.")
                    with st.expander("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞", expanded=False):
                        st.dataframe(merged_cached.head(20))

        # –í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —à–∞–≥–µ 2
        if df_second is not None and st.session_state.get("use_merge") and "merged_df_cache" in st.session_state:
            df = st.session_state["merged_df_cache"].copy()
            active_filters = active_filters_main + active_filters_second + [f"MERGE ({st.session_state.get('merge_how', 'inner')})"]
        else:
            df = df_main.copy()
            active_filters = active_filters_main + active_filters_second

        # –°—á–∏—Ç–∞–µ–º "–∏—Å—Ö–æ–¥–Ω—ã–µ" —Å—Ç—Ä–æ–∫–∏/—Å—Ç–æ–ª–±—Ü—ã –¥–ª—è KPI —à–∞–≥–∞ 2:
        # –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è merge –∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Å—á–∏—Ç–∞–µ–º –ø–æ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É,
        # –∏–Ω–∞—á–µ ‚Äî –ø–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–º—É –æ—Å–Ω–æ–≤–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.
        if df_second is not None and st.session_state.get("use_merge") and "merged_df_cache" in st.session_state:
            base_kpi_df = st.session_state["merged_df_cache"]
        else:
            base_kpi_df = df_main

        st.session_state["analysis_kpi_rows_origin"] = int(base_kpi_df.shape[0])
        st.session_state["analysis_kpi_cols_origin"] = int(base_kpi_df.shape[1])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—ã –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–µ—Å—Å–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞ —à–∞–≥–µ 3
        st.session_state["df_analysis"] = df
        st.session_state["active_filters_all"] = active_filters
        st.session_state["filter_config"] = filter_config
        st.session_state["filter_config_second"] = filter_config_second
        st.session_state["analysis_cols_default"] = df.columns.tolist()

    else:
        # –®–∞–≥ 3 –∏ –¥–∞–ª–µ–µ: –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —à–∞–≥–µ 2 –¥–∞—Ç–∞—Å–µ—Ç
        df = st.session_state.get("df_analysis", df_raw.copy())
        active_filters = st.session_state.get("active_filters_all", [])
        filter_config = st.session_state.get("filter_config", default_filter_cfg)
        filter_config_second = st.session_state.get("filter_config_second", default_filter2_cfg)
    # –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –∫–æ–ª–æ–Ω–æ–∫
    st.sidebar.header("–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    # –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    all_columns = df.columns.tolist()
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ ID‚Äë–∫–æ–ª–æ–Ω–∫–∏ –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
    id_columns_all = detect_id_columns(df)
    # –í—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –±–µ–∑ ID
    numeric_cols_all = [c for c in df.select_dtypes(include=[np.number]).columns if c not in id_columns_all]
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ (–¥–ª—è —Ç–æ–ø‚Äë10 –ø–æ CV)
    cv_scores: dict[str, float] = {}
    for c in numeric_cols_all:
        col = df[c].dropna()
        if len(col) == 0:
            cv_scores[c] = 0.0
            continue
        mean_val = col.mean()
        std_val = col.std(ddof=1)
        cv_scores[c] = abs(std_val / mean_val) if mean_val not in (0, np.nan) and not np.isnan(mean_val) else 0.0
    # –ë–ª–æ–∫ –±—ã—Å—Ç—Ä—ã—Ö –ø—Ä–µ—Å–µ—Ç–æ–≤ –≤—ã–±–æ—Ä–∞
    st.sidebar.markdown("**–ë—ã—Å—Ç—Ä—ã–π –≤—ã–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫:**")
    preset_choice = st.sidebar.selectbox(
        "–¢–∏–ø –ø—Ä–µ—Å–µ—Ç–∞",
        ["‚Äî", "–ß–∏—Å–ª–æ–≤—ã–µ –±–µ–∑ ID", "–¢–æ–ø‚Äë10 –ø–æ CV"],
        help="–ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –∫–æ–ª–æ–Ω–æ–∫",
        key="preset_choice",
    )
    preset_cols: list[str] | None = None
    if preset_choice == "–ß–∏—Å–ª–æ–≤—ã–µ –±–µ–∑ ID":
        preset_cols = numeric_cols_all.copy()
    elif preset_choice == "–¢–æ–ø‚Äë10 –ø–æ CV":
        # –í—ã–±–∏—Ä–∞–µ–º –¥–æ 10 –∫–æ–ª–æ–Ω–æ–∫ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º CV
        sorted_cols = sorted(cv_scores.keys(), key=lambda k: cv_scores[k], reverse=True)
        preset_cols = sorted_cols[: min(10, len(sorted_cols))]
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏
    analysis_default = preset_cols if preset_cols else all_columns
    analysis_cols = st.sidebar.multiselect(
        "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏",
        options=all_columns,
        default=analysis_default,
    ) or analysis_default

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º ID‚Äë–∫–æ–ª–æ–Ω–∫–∏, —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞
    id_columns = detect_id_columns(df[analysis_cols])
    numeric_cols = [c for c in df[analysis_cols].select_dtypes(include=[np.number]).columns if c not in id_columns]
    categorical_cols = [c for c in analysis_cols if c not in numeric_cols]


    # –®–∞–≥ 3: –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AI-–ø–æ–º–æ—â–Ω–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if step == 3:
        st.subheader("–®–∞–≥ 3. AI-–ø–æ–º–æ—â–Ω–∏–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")

        mode_label_map = {
            "off": "–û—Ç–∫–ª—é—á–µ–Ω–æ",
            "local": "–¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏",
            "openai": "OpenAI (—á–µ—Ä–µ–∑ API)",
        }
        reverse_map = {v: k for k, v in mode_label_map.items()}

        current_mode = st.session_state.get("ai_mode", "local")
        ui_label = mode_label_map.get(current_mode, "–¢–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏")

        chosen_label = st.radio(
            "–†–µ–∂–∏–º AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤",
            list(mode_label_map.values()),
            index=list(mode_label_map.values()).index(ui_label),
            horizontal=False,
        )
        st.session_state["ai_mode"] = reverse_map[chosen_label]

        if st.session_state["ai_mode"] == "openai":
            st.session_state["ai_api_key"] = st.text_input(
                "API –∫–ª—é—á OpenAI",
                type="password",
                value=st.session_state.get("ai_api_key", ""),
            )
            st.caption("–ö–ª—é—á —Ö—Ä–∞–Ω–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ø–∞–º—è—Ç–∏ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏ –∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–∞–π–ª.")

            if st.button("üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–ª—é—á OpenAI"):
                try:
                    import openai  # type: ignore

                    test_prompt = "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–æ–π 'OK', –µ—Å–ª–∏ –≤–∏–¥–∏—à—å —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å."
                    # –ù–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç (openai>=1.x)
                    if hasattr(openai, "OpenAI"):
                        client = openai.OpenAI(api_key=st.session_state["ai_api_key"])
                        _ = client.chat.completions.create(
                            model="gpt-4o-mini",
                            messages=[
                                {"role": "system", "content": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."},
                                {"role": "user", "content": test_prompt},
                            ],
                            max_tokens=5,
                        )
                    else:
                        openai.api_key = st.session_state["ai_api_key"]
                        _ = openai.ChatCompletion.create(
                            model="gpt-4o-mini",
                            messages=[
                                {"role": "system", "content": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."},
                                {"role": "user", "content": test_prompt},
                            ],
                            max_tokens=5,
                        )
                    st.success("–ö–ª—é—á –≤—ã–≥–ª—è–¥–∏—Ç —Ä–∞–±–æ—á–∏–º: –∑–∞–ø—Ä–æ—Å –∫ OpenAI –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
                except Exception as e:  # pragma: no cover
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ OpenAI: {e}")

        # –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-—Å–≤–æ–¥–æ–∫
        st.checkbox(
            "–ê–≤—Ç–æ‚Äë–≥–µ–Ω–µ—Ä–∞—Ü–∏—è AI‚Äë—Å–≤–æ–¥–æ–∫",
            value=st.session_state.get("auto_ai_call", False),
            key="auto_ai_call",
            help=(
                "–ö–æ–≥–¥–∞ –≤—ã–∫–ª—é—á–µ–Ω–æ, AI-—Å–≤–æ–¥–∫–∏ –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–∞—è"
                " —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–≤–æ–¥–∫–∞. –í–∫–ª—é—á–∏—Ç–µ, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∞—Ç—å AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏."
            ),
        )

        st.info(
            "–ù–∞—á–∏–Ω–∞—è —Å **—à–∞–≥–∞ 4**, –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—é–º–µ (EDA, –±–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã, A/B –∏ –¥—Ä.) "
            "–º–æ–≥—É—Ç –¥–æ–ø–æ–ª–Ω—è—Ç—å—Å—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ AI –ø—Ä–∏ –≤–∫–ª—é—á—ë–Ω–Ω–æ–π –∞–≤—Ç–æ‚Äë–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
        )
    # –ï—Å–ª–∏ –º—ã –Ω–∞ —à–∞–≥–µ 2 ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏,
    # –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –≤–∫–ª–∞–¥–æ–∫. –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ —à–∞–≥–µ 4.
    if step == 2:
        st.subheader("–®–∞–≥ 2. –§–∏–ª—å—Ç—Ä—ã –∏ –∫–æ–Ω—Ñ–∏–≥ ‚Äî –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
        c1, c2, c3, c4 = st.columns(4)
        origin_rows = st.session_state.get("analysis_kpi_rows_origin", df_raw.shape[0])
        origin_cols = st.session_state.get("analysis_kpi_cols_origin", df_raw.shape[1])
        c1.metric("–°—Ç—Ä–æ–∫ (–∏—Å—Ö–æ–¥–Ω–æ)", origin_rows)
        c2.metric("–°—Ç—Ä–æ–∫ (—Ñ–∏–ª—å—Ç—Ä)", df.shape[0])
        c3.metric("–°—Ç–æ–ª–±—Ü–æ–≤", df.shape[1])
        c4.metric("–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤", len(active_filters))

        with st.expander("üîé –¢–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã", expanded=bool(active_filters)):
            if active_filters:
                st.markdown("\n".join(f"- {x}" for x in active_filters))
            else:
                st.info("–§–∏–ª—å—Ç—Ä—ã –Ω–µ –∑–∞–¥–∞–Ω—ã.")

        with st.expander("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", expanded=True):
            st.dataframe(df.head(50))

        if id_columns:
            st.caption(f"ID-–ø–æ–¥–æ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —á–∏—Å–ª–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏): {', '.join(id_columns)}")

        st.info(
            "–ö–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ñ–∏–ª—å—Ç—Ä–æ–≤, –≤—ã–±–æ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö AI-–Ω–∞—Å—Ç—Ä–æ–µ–∫, "
            "–Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É ¬´–í–ø–µ—Ä—ë–¥ ‚û°Ô∏è¬ª –≤ –±–ª–æ–∫–µ —à–∞–≥–æ–≤ –≤—ã—à–µ, —á—Ç–æ–±—ã –ø–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ–ª–Ω–æ–º—É –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É (–®–∞–≥ 4)."
        )
        return

    # –ï—Å–ª–∏ –º—ã –µ—â—ë –Ω–µ –¥–æ—à–ª–∏ –¥–æ —à–∞–≥–∞ 4 ‚Äî –≤–∫–ª–∞–¥–∫–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º
    if step < 4:
        return

    # ------------------------- TABS -------------------------
    (tab_data, tab_stats, tab_corr, tab_ts, tab_outliers, tab_feats, tab_cats,
     tab_groups, tab_ab, tab_scenarios, tab_dict, tab_report) = st.tabs(
        ["üìÅ –î–∞–Ω–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ", "üìä –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏",
         "‚è± –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã", "‚ö†Ô∏è –í—ã–±—Ä–æ—Å—ã", "üß© –§–∏—á–∏", "üè∑ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏",
         "üìê –ì—Ä—É–ø–ø—ã / Pivot", "üß™ A/B", "üîÅ –°—Ü–µ–Ω–∞—Ä–∏–∏", "üìö –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫", "üì§ –û—Ç—á—ë—Ç"]
    )

    # ---- DATA
    with tab_data:
        st.subheader("üìÅ –î–∞–Ω–Ω—ã–µ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)")
        c1, c2, c3, c4 = st.columns(4)
        origin_rows = st.session_state.get("analysis_kpi_rows_origin", df_raw.shape[0])
        origin_cols = st.session_state.get("analysis_kpi_cols_origin", df_raw.shape[1])
        c1.metric("–°—Ç—Ä–æ–∫ (–∏—Å—Ö–æ–¥–Ω–æ)", origin_rows)
        c2.metric("–°—Ç—Ä–æ–∫ (—Ñ–∏–ª—å—Ç—Ä)", df.shape[0])
        c3.metric("–°—Ç–æ–ª–±—Ü–æ–≤", df.shape[1])
        c4.metric("–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤", len(active_filters))

        with st.expander("üîé –¢–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã", expanded=bool(active_filters)):
            if active_filters:
                st.markdown("\n".join(f"- {x}" for x in active_filters))
            else:
                st.info("–§–∏–ª—å—Ç—Ä—ã –Ω–µ –∑–∞–¥–∞–Ω—ã.")

        with st.expander("–¢–∞–±–ª–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö", expanded=True):
            st.dataframe(df.head(100))

        if id_columns:
            st.caption(f"ID-–ø–æ–¥–æ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —á–∏—Å–ª–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏): {', '.join(id_columns)}")

        st.download_button("üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CSV)",
                           data=df.to_csv(index=False).encode("utf-8"),
                           file_name="filtered_data.csv", mime="text/csv")

        with st.expander("üßº –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (Data Quality)", expanded=False):
            dq = compute_data_quality_table(df[analysis_cols])
            st.dataframe(dq)
            # –í—ã–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –¥–æ–ª–µ–π –ø—Ä–æ–ø—É—Å–∫–æ–≤
            hi = dq[dq["missing_%"] > 30]
            if hi.empty:
                st.info("–ö–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –±–æ–ª–µ–µ 30% –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
            else:
                st.markdown("**–ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ > 30%**")
                st.dataframe(hi)
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º –∏ —Å–º–µ—à–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
            with st.expander("üõ† –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º", expanded=False):
                recs: list[str] = []
                for _, row in dq.iterrows():
                    if row.get("type_suggestion"):
                        recs.append(
                            f"‚Ä¢ {row['column']} ‚Äî –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ {row['type_suggestion']} (—Ç–µ–∫—É—â–∏–π —Ç–∏–ø {row['dtype']})"
                        )
                    if row.get("mixed_types"):
                        recs.append(
                            f"‚Ä¢ {row['column']} ‚Äî —Å–º–µ—à–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –∑–Ω–∞—á–µ–Ω–∏–π, –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç"
                        )
                if recs:
                    st.markdown("\n".join(recs))
                else:
                    st.info("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø–∞–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

            # –ö—Ä–∞—Ç–∫–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö + AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ –∫–Ω–æ–ø–∫–µ
            issues = []
            many_missing = dq[dq["missing_%"] > 30]["column"].tolist()
            if many_missing:
                issues.append(
                    "–ö–æ–ª–æ–Ω–∫–∏ —Å >30% –ø—Ä–æ–ø—É—Å–∫–æ–≤: " + ", ".join(many_missing) + " ‚Äî —Å—Ç–æ–∏—Ç –ø–æ–¥—É–º–∞—Ç—å –æ–± –∏—Å–∫–ª—é—á–µ–Ω–∏–∏ –∏–ª–∏ –∏–º–ø—É—Ç–∞—Ü–∏–∏."
                )
            const_cols = dq[dq["is_constant"] == True]["column"].tolist()
            if const_cols:
                issues.append(
                    "–ö–æ–ª–æ–Ω–∫–∏-–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã: " + ", ".join(const_cols) + " ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –Ω–µ—Å—É—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
                )
            type_suspicious = dq[dq["type_suggestion"].notna()]["column"].tolist()
            if type_suspicious:
                issues.append(
                    "–ö–æ–ª–æ–Ω–∫–∏ —Å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º —Ç–∏–ø–æ–º (–≤–æ–∑–º–æ–∂–µ–Ω numeric/datetime): " + ", ".join(type_suspicious) + "."
                )

            local_dq_summary = " ".join(issues) if issues else "–°–µ—Ä—å—ë–∑–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
            render_ai_block(
                local_dq_summary,
                "ü§ñ AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö",
                "dq_overall",
                extra_prompt="–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö –∏ –≤–æ–∑–º–æ–∂–Ω—ã–º —à–∞–≥–∞–º –ø–æ –æ—á–∏—Å—Ç–∫–µ."
            )
        st.subheader("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥ –∞–Ω–∞–ª–∏–∑–∞")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ —Ç–æ–ª—å–∫–æ —Ñ–∏–ª—å—Ç—Ä—ã, –Ω–æ –∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞ (–µ—Å–ª–∏ —É–∂–µ –≤—ã–±–∏—Ä–∞–ª–∏—Å—å)
        cfg_dict = {"filters": filter_config}
        if "report_num_cols" in st.session_state:
            cfg_dict["selected_report_columns"] = st.session_state.get("report_num_cols", [])
        cfg_bytes = json.dumps(cfg_dict, ensure_ascii=False, indent=2).encode("utf-8")
        st.download_button(
            "üì• –°–∫–∞—á–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥ (JSON)",
            data=cfg_bytes,
            file_name="analysis_config.json",
            mime="application/json",
        )

    # ---- BASIC STATS
    with tab_stats:
        st.header("1Ô∏è‚É£ –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
        if not numeric_cols:
            st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤.")
        else:
            col = st.selectbox("–ß–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞", numeric_cols)
            s = df[col].dropna()
            stats_row = describe_basic_stats(s)

            with st.expander("üìä –¢–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫", expanded=True):
                st.write(pd.DataFrame(stats_row, index=["–∑–Ω–∞—á–µ–Ω–∏—è"]).T)

            norm_res = normality_test(s)
            mask_iqr, *_ = detect_outliers_iqr(s, k=1.5)
            n_out = int(mask_iqr.sum())

            with st.expander("üéØ –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ", expanded=False):
                n = stats_row["count"]
                if n > 1:
                    ci_level = st.slider("–£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è", 0.80, 0.99, 0.95, 0.01)
                    mean_v, std_v = stats_row["mean"], stats_row["std"]
                    se = std_v / np.sqrt(n) if n else np.nan
                    if not np.isnan(se) and se > 0:
                        alpha = 1 - ci_level
                        t_crit = stats.t.ppf(1 - alpha / 2, df=n - 1)
                        ci_low, ci_high = mean_v - t_crit * se, mean_v + t_crit * se
                        st.write({"mean": mean_v, "n": n, "ci_level": ci_level, "ci_low": ci_low, "ci_high": ci_high})
                else:
                    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CI.")

            with st.expander("üìà –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", expanded=True):
                if len(s) > 0:
                    fig = px.histogram(s, nbins=30, marginal="box", title=f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {col}")
                    fig.update_layout(margin=dict(t=40, r=20, b=40, l=40))
                    st.plotly_chart(fig, use_container_width=True)

            with st.expander("‚öñÔ∏è –¢–µ—Å—Ç –®–∞–ø–∏—Ä–æ‚Äì–£–∏–ª–∫–∞", expanded=False):
                st.write("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö." if norm_res is None else norm_res)

            with st.expander("üßæ –ë–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –º–µ—Ç—Ä–∏–∫–µ", expanded=True):
                local_summary = business_summary_for_series(col, stats_row, norm_res, n_out)
                render_ai_block(
                    local_summary,
                    "ü§ñ AI-—Ä–µ–∑—é–º–µ –ø–æ –º–µ—Ç—Ä–∏–∫–µ",
                    f"metric_{col}",
                    extra_prompt=(
                        f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –º–µ—Ç—Ä–∏–∫–µ '{col}'. "
                        "–û–ø–∏—à–∏ –µ—ë –ø–æ–≤–µ–¥–µ–Ω–∏–µ, —Ä–∏—Å–∫–∏, –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞."
                    ),
                )

            with st.expander("üìä EDA –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", expanded=False):
                if not categorical_cols:
                    st.info("–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")
                else:
                    gcol = st.selectbox(
                        "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏",
                        categorical_cols,
                        key="eda_cat_col",
                    )
                    seg_metrics = st.multiselect(
                        "–ß–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º",
                        numeric_cols,
                        default=[col] if col in numeric_cols else numeric_cols[:1],
                        key="eda_cat_metrics",
                    )
                    if not seg_metrics:
                        st.info("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É.")
                    else:
                        agg_df = (
                            df.groupby(gcol)[seg_metrics]
                            .agg(["mean", "median", "count"])
                            .reset_index()
                        )
                        st.dataframe(agg_df.head(50))

                        plot_metric = st.selectbox(
                            "–ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º)",
                            seg_metrics,
                            key="eda_cat_plot_metric",
                        )
                        try:
                            mean_by_cat = (
                                df.groupby(gcol)[plot_metric].mean().reset_index()
                            )
                            fig_seg = px.bar(
                                mean_by_cat,
                                x=gcol,
                                y=plot_metric,
                                title=f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {plot_metric} –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º {gcol}",
                            )
                            st.plotly_chart(fig_seg, use_container_width=True)
                        except Exception as e:
                            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {e}")

                        # –ù–µ–±–æ–ª—å—à–æ–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
                        top_rows = agg_df.sort_values((seg_metrics[0], "mean"), ascending=False).head(3)
                        bottom_rows = agg_df.sort_values((seg_metrics[0], "mean"), ascending=True).head(3)
                        text_lines = ["EDA –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:", f"- –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {gcol}"]
                        text_lines.append(f"- –ú–µ—Ç—Ä–∏–∫–∏: {', '.join(seg_metrics)}")
                        text_lines.append("–¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å—Ä–µ–¥–Ω–µ–π –≤–µ–ª–∏—á–∏–Ω–µ –ø–µ—Ä–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏:")
                        for _, r in top_rows.iterrows():
                            text_lines.append(f"  ‚Ä¢ {r[gcol]}")
                        text_lines.append("–ê–Ω—Ç–∏-—Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
                        for _, r in bottom_rows.iterrows():
                            text_lines.append(f"  ‚Ä¢ {r[gcol]}")
                        seg_summary = "\n".join(text_lines)

                        # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ + AI-—Ä–µ–∑—é–º–µ –ø–æ –∫–Ω–æ–ø–∫–µ
                        render_ai_block(
                            seg_summary,
                            "ü§ñ AI-—Ä–µ–∑—é–º–µ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º",
                            f"segments_{gcol}",
                            extra_prompt=(
                                "–°–¥–µ–ª–∞–π –±–∏–∑–Ω–µ—Å-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ —ç—Ç–∏–º —Å–µ–≥–º–µ–Ω—Ç–∞–º: "
                                "–≥–¥–µ –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—à–µ/–Ω–∏–∂–µ, –∫–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –º–æ–∂–Ω–æ –≤—ã–¥–≤–∏–Ω—É—Ç—å –∏ —á—Ç–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ A/B."
                            ),
                        )



    # ---- CORRELATIONS
    with tab_corr:
        st.header("2Ô∏è‚É£ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        if numeric_cols and len(numeric_cols) >= 2:
            with st.expander("–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π", expanded=True):
                method = st.selectbox("–ú–µ—Ç–æ–¥", ["pearson", "spearman"], format_func=lambda m: "–ü–∏—Ä—Å–æ–Ω" if m=="pearson" else "–°–ø–∏—Ä–º–µ–Ω")
                cm = compute_corr_matrix_cached(df, tuple(numeric_cols), method)
                st.dataframe(cm)
                fig = px.imshow(cm, text_auto=False, color_continuous_scale="RdBu", zmin=-1, zmax=1,
                                title="–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞")
                st.plotly_chart(fig, use_container_width=True)


            with st.expander("–°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏", expanded=False):
                thr = st.slider("–ü–æ—Ä–æ–≥ |r|", 0.0, 1.0, 0.7, 0.05)
                strong = get_strong_correlations(cm, threshold=thr)
                st.write("–ù–µ—Ç –ø–∞—Ä." if strong.empty else strong[["feature_1", "feature_2", "r"]])

            with st.expander("–ü–∞—Ä–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –∏ scatter", expanded=False):
                c1 = st.selectbox("–ü–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞", numeric_cols, key="corr_c1")
                c2 = st.selectbox("–í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞", numeric_cols, key="corr_c2")
                if c1 != c2:
                    x, y = df[c1].dropna(), df[c2].dropna()
                    idx = x.index.intersection(y.index)
                    x, y = x.loc[idx], y.loc[idx]
                    if len(x) >= 3:
                        pr, pp = stats.pearsonr(x, y)
                        sr, sp = stats.spearmanr(x, y)
                        st.write({"pearson_r": float(pr), "pearson_p": float(pp), "spearman_rho": float(sr), "spearman_p": float(sp)})
                        x1, y1 = maybe_downsample_xy(x, y, 10000)
                        scat = px.scatter(x=x1, y=y1, labels={"x": c1, "y": c2}, title=f"Scatter: {c1} vs {c2}")
                        st.plotly_chart(scat, use_container_width=True)
                        local_corr_summary = business_summary_for_correlation(c1, c2, pr, pp)
                        render_ai_block(
                            local_corr_summary,
                            "ü§ñ –ü–æ–ø—Ä–æ—Å–∏—Ç—å AI –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç—É —Å–≤—è–∑—å",
                            f"corr_{c1}_{c2}",
                            extra_prompt=(
                                f"–ü–æ—è—Å–Ω–∏, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É '{c1}' –∏ '{c2}' –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞. "
                                "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞."
                            ),
                        )
                else:
                    st.info("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏.")
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")

    # ---- TIME SERIES
    with tab_ts:
        st.header("3Ô∏è‚É£ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã")
        date_col = st.selectbox("–î–∞—Ç–∞", ["<–Ω–µ—Ç>"] + analysis_cols)
        if date_col != "<–Ω–µ—Ç>" and numeric_cols:
            value_col = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols)

            # KPI
            try:
                _, feats = generate_ts_features(df, date_col, value_col, window=7, spike_thresh_pct=50.0)
                c1, c2, c3 = st.columns(3)
                c1.metric("–¢–æ—á–µ–∫", feats.get("n_points", 0))
                c2.metric("Œî, %", f"{feats.get('change_pct', np.nan):.1f}" if not np.isnan(feats.get("change_pct", np.nan)) else "n/a")
                c3.metric("CV", f"{feats.get('cv', np.nan):.2f}" if not np.isnan(feats.get('cv', np.nan)) else "n/a")
                st.caption("CV ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ (std/mean); –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–∏.")
            except Exception:
                pass

            with st.expander("–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ (Plotly)", expanded=True):
                method = st.selectbox("–ú–µ—Ç–æ–¥", ["–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ", "EWMA", "–°–∫–æ–ª—å–∑—è—â–∞—è –º–µ–¥–∏–∞–Ω–∞"])
                window = st.slider("–û–∫–Ω–æ/span", 3, 60, 7, 1)
                show_spikes = st.checkbox("–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã (—Å–ø–∞–π–∫–∏)", value=True)
                spike_thresh = st.slider("–ü–æ—Ä–æ–≥ –∏–∑–º–µ–Ω–µ–Ω–∏—è, % –¥–ª—è —Å–ø–∞–π–∫–∞", 5.0, 200.0, 50.0, 5.0)
                if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä—è–¥"):
                    try:
                        ts_data, gf = generate_ts_features(
                            df, date_col, value_col, window=window, spike_thresh_pct=spike_thresh
                        )
                        fig_ts = plot_ts_plotly(ts_data, date_col, value_col, method, window)
                        if show_spikes:
                            spikes = ts_data[ts_data["spike_flag"]]
                            if not spikes.empty:
                                fig_ts.add_scatter(
                                    x=spikes[date_col],
                                    y=spikes[value_col],
                                    mode="markers",
                                    name="–°–ø–∞–π–∫–∏",
                                    marker=dict(symbol="circle-open", size=9),
                                )
                        st.plotly_chart(fig_ts, use_container_width=True)
                        with st.expander("–ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", expanded=False):
                            st.json(gf)
                        ts_local_summary = business_summary_for_ts(gf)
                        render_ai_block(
                            ts_local_summary,
                            "ü§ñ AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä—è–¥—É",
                            f"ts_{value_col}",
                            extra_prompt=(
                                f"–°–¥–µ–ª–∞–π –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —Ä—è–¥—É –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ '{value_col}'. "
                                "–û–ø–∏—à–∏ —Ç—Ä–µ–Ω–¥, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã."
                            ),
                        )
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞: {e}")


            with st.expander("–ü—Ä–æ–≥–Ω–æ–∑ (ARIMA)", expanded=False):
                if ARIMA is None:
                    st.info("ARIMA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ statsmodels (tsa.arima).")
                else:
                    # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞
                    horizon = st.slider("–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤)", 5, 60, 14, 1)
                    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤: –¥–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ-–ø–æ–¥–±–æ—Ä
                    order_label = st.selectbox(
                        "–ü–æ—Ä—è–¥–æ–∫ –º–æ–¥–µ–ª–∏ ARIMA (p, d, q)",
                        ["(1, 1, 0)", "(1, 1, 1)", "(2, 1, 1)", "auto (–ø–æ–¥–±–æ—Ä)"],
                    )
                    # –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    order_map = {
                        "(1, 1, 0)": (1, 1, 0),
                        "(1, 1, 1)": (1, 1, 1),
                        "(2, 1, 1)": (2, 1, 1),
                    }
                    if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):
                        try:
                            # –ü–æ–¥–±–æ—Ä –º–æ–¥–µ–ª–∏
                            if order_label == "auto (–ø–æ–¥–±–æ—Ä)":
                                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ARIMA
                                hist_df, fc_df, auto_order = ts_forecast_auto_arima(
                                    df, date_col, value_col, horizon=int(horizon)
                                )
                                order_desc = f"–ê–≤—Ç–æ‚Äë–≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫: {auto_order}"
                            else:
                                # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–∞
                                hist_df, fc_df = ts_forecast_arima(
                                    df,
                                    date_col,
                                    value_col,
                                    horizon=int(horizon),
                                    order=order_map[order_label],
                                )
                                auto_order = order_map[order_label]
                                order_desc = f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫: {auto_order}"
                            # –†–∏—Å—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑
                            fig_f = go.Figure()
                            fig_f.add_trace(
                                go.Scatter(
                                    x=hist_df["date"],
                                    y=hist_df["value"],
                                    mode="lines",
                                    name="–ò—Å—Ç–æ—Ä–∏—è",
                                )
                            )
                            fig_f.add_trace(
                                go.Scatter(
                                    x=fc_df["date"],
                                    y=fc_df["forecast"],
                                    mode="lines",
                                    name="–ü—Ä–æ–≥–Ω–æ–∑",
                                )
                            )
                            fig_f.add_trace(
                                go.Scatter(
                                    x=list(fc_df["date"]) + list(fc_df["date"][::-1]),
                                    y=list(fc_df["upper"]) + list(fc_df["lower"][::-1]),
                                    fill="toself",
                                    mode="lines",
                                    name="–î–ò –ø—Ä–æ–≥–Ω–æ–∑–∞ (90%)",
                                    line=dict(width=0),
                                    opacity=0.3,
                                )
                            )
                            fig_f.update_layout(
                                title=f"–ü—Ä–æ–≥–Ω–æ–∑ ARIMA –¥–ª—è {value_col}",
                                hovermode="x unified",
                                margin=dict(t=40, r=20, b=40, l=40),
                            )
                            st.plotly_chart(fig_f, use_container_width=True)
                            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                            st.caption(order_desc)
                            # --- –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ ---
                            try:
                                # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è CV: —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ, –ø—Ä–∏–≤–æ–¥–∏–º –∫ —á–∏—Å–ª–∞–º
                                d_ts = df[[date_col, value_col]].dropna().copy()
                                d_ts[date_col] = pd.to_datetime(d_ts[date_col])
                                d_ts = d_ts.sort_values(date_col)
                                y_all = pd.to_numeric(d_ts[value_col], errors="coerce").dropna()
                                h = int(horizon)
                                # cross-validation —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ (‚â•2*h)
                                if ARIMA is not None and len(y_all) >= 2 * h and h > 0:
                                    train = y_all.iloc[:-h]
                                    test = y_all.iloc[-h:]
                                    # –ü–æ—Ä—è–¥–æ–∫ CV ‚Äî —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                                    order_cv = auto_order
                                    cv_model = ARIMA(train, order=order_cv)
                                    cv_res = cv_model.fit()
                                    cv_fc = cv_res.forecast(steps=h)
                                    # –ú–µ—Ç—Ä–∏–∫–∏
                                    mae = float(np.mean(np.abs(cv_fc.values - test.values)))
                                    # MAPE: –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                                    mape_vals = []
                                    for yy_true, yy_pred in zip(test.values, cv_fc.values):
                                        if yy_true != 0:
                                            mape_vals.append(abs((yy_pred - yy_true) / yy_true))
                                    mape = float(np.mean(mape_vals)) if mape_vals else float('nan')
                                    rmse = float(np.sqrt(np.mean((cv_fc.values - test.values)**2)))
                                    st.markdown(
                                        f"**–ö—Ä–æ—Å—Å‚Äë–≤–∞–ª–∏–¥–∞—Ü–∏—è:** MAE‚âà{mae:.3g}, MAPE‚âà{mape*100:.2f}%, RMSE‚âà{rmse:.3g}"
                                    )
                            except Exception:
                                pass

                            # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑—É + AI –ø–æ –∫–Ω–æ–ø–∫–µ
                            try:
                                last_hist = float(hist_df["value"].iloc[-1])
                                last_fc = float(fc_df["forecast"].iloc[-1])
                                change_abs = last_fc - last_hist
                                change_pct = (change_abs / last_hist * 100.0) if last_hist != 0 else float("nan")
                                fc_lower = float(fc_df["lower"].iloc[-1])
                                fc_upper = float(fc_df["upper"].iloc[-1])
                                direction = "—Ä–∞—Å—Ç—ë—Ç" if change_abs > 0 else "—Å–Ω–∏–∂–∞–µ—Ç—Å—è" if change_abs < 0 else "–æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–∂–Ω–µ–º —É—Ä–æ–≤–Ω–µ"
                                parts = [
                                    f"–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ {value_col} –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç {horizon} —Ç–æ—á–µ–∫: "
                                    f"–∫–ª—é—á–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å {direction} –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è.",
                                    f"–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {last_hist:.3g}, –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ: {last_fc:.3g} "
                                    f"(–¥–∏–∞–ø–∞–∑–æ–Ω [{fc_lower:.3g}; {fc_upper:.3g}]).",
                                ]
                                if not np.isnan(change_pct):
                                    sign = "+" if change_pct >= 0 else ""
                                    parts.append(f"–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ñ–∞–∫—Ç—É: {sign}{change_pct:.1f}%.")
                                local_fc_summary = " ".join(parts)
                            except Exception:
                                local_fc_summary = f"–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ {value_col} –ø–æ—Å—Ç—Ä–æ–µ–Ω."

                            render_ai_block(
                                local_fc_summary,
                                "ü§ñ AI-—Ä–µ–∑—é–º–µ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑—É",
                                f"ts_forecast_{value_col}",
                                extra_prompt="–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π —ç—Ç–æ—Ç ARIMA-–ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞: —Ç—Ä–µ–Ω–¥, —Ä–∏—Å–∫–∏, –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å –Ω–∞–¥—ë–∂–Ω–∞."
                            )
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞: {e}")

            with st.expander("–†–µ—Å–µ–º–ø–ª–∏–Ω–≥", expanded=False):
                freq_label = st.selectbox("–ß–∞—Å—Ç–æ—Ç–∞", ["D (–¥–Ω–∏)", "W (–Ω–µ–¥–µ–ª–∏)", "M (–º–µ—Å—è—Ü—ã)"])
                agg_func = st.selectbox("–ê–≥—Ä–µ–≥–∞—Ç", ["mean", "sum", "max", "min"])
                f_map = {"D (–¥–Ω–∏)": "D", "W (–Ω–µ–¥–µ–ª–∏)": "W", "M (–º–µ—Å—è—Ü—ã)": "M"}
                if st.button("–†–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å"):
                    try:
                        d = df[[date_col, value_col]].dropna().copy()
                        d[date_col] = pd.to_datetime(d[date_col])
                        d = d.sort_values(date_col).set_index(date_col)
                        res = getattr(d[value_col].resample(f_map[freq_label]), agg_func)().reset_index()
                        fig = px.line(res, x=date_col, y=value_col, title=f"–†–µ—Å–µ–º–ø–ª–∏–Ω–≥ ({freq_label}, {agg_func})")
                        st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–∞: {e}")

            with st.expander("STL-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ", expanded=False):
                if STL is None:
                    st.info("STL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ statsmodels.")
                else:
                    period = st.number_input("–ü–µ—Ä–∏–æ–¥ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏", 2, 365, 7, 1)
                    if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å STL"):
                        try:
                            with st.spinner("–°—á–∏—Ç–∞–µ–º STL-–¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é..."):
                                d = df[[date_col, value_col]].dropna().copy()
                                d[date_col] = pd.to_datetime(d[date_col])
                                d = d.sort_values(date_col).set_index(date_col)[value_col].asfreq("D").interpolate()
                                res = STL(d, period=int(period), robust=True).fit()
                                comp = pd.DataFrame({"date": d.index, "observed": d.values,
                                                     "trend": res.trend, "seasonal": res.seasonal, "resid": res.resid})
                            st.plotly_chart(make_stl_figure(comp), use_container_width=True)
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ STL: {e}")

            with st.expander("–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –ø–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—é", expanded=False):
                data = df[[date_col, value_col]].dropna().copy()
                if len(data):
                    data[date_col] = pd.to_datetime(data[date_col])
                    gran = st.selectbox("–ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å", ["–î–Ω–∏ –Ω–µ–¥–µ–ª–∏", "–ú–µ—Å—è—Ü—ã", "–ß–∞—Å—ã"])
                    if gran == "–î–Ω–∏ –Ω–µ–¥–µ–ª–∏":
                        data["key"] = data[date_col].dt.dayofweek
                        name_map = {0:"–ü–Ω",1:"–í—Ç",2:"–°—Ä",3:"–ß—Ç",4:"–ü—Ç",5:"–°–±",6:"–í—Å"}
                    elif gran == "–ú–µ—Å—è—Ü—ã":
                        data["key"] = data[date_col].dt.month
                        name_map = {1:"–Ø–Ω–≤",2:"–§–µ–≤",3:"–ú–∞—Ä",4:"–ê–ø—Ä",5:"–ú–∞–π",6:"–ò—é–Ω",7:"–ò—é–ª",8:"–ê–≤–≥",9:"–°–µ–Ω",10:"–û–∫—Ç",11:"–ù–æ—è",12:"–î–µ–∫"}
                    else:
                        data["key"] = data[date_col].dt.hour
                        name_map = None
                    grp = data.groupby("key")[value_col].mean().rename("value")
                    idx = sorted(grp.index)
                    show = pd.DataFrame({"group": [name_map.get(i, str(i)) if name_map else str(i) for i in idx],
                                         "value": grp.loc[idx].values})
                    fig = px.bar(show, x="group", y="value", title=f"–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å ({gran})")
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö.")

            with st.expander("–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è (ACF)", expanded=False):
                data = df[[date_col, value_col]].dropna().copy()
                if len(data):
                    data = data.sort_values(date_col)
                    s = data[value_col]
                    max_lag = st.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∞–≥", 1, max(2, min(60, len(s)-1)), min(20, len(s)-1))
                    if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å ACF"):
                        lags, acf_vals = compute_acf(s, max_lag)
                        st.plotly_chart(
                            px.bar(
                                pd.DataFrame({"lag": lags, "acf": acf_vals}),
                                x="lag",
                                y="acf",
                                title=f"ACF –¥–ª—è {value_col}",
                            ),
                            use_container_width=True,
                        )
                        # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —Å–µ–∑–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –ø–æ –ø–∏–∫—É ACF
                        if len(acf_vals) > 2:
                            acf_series = pd.Series(acf_vals, index=lags)
                            acf_series = acf_series[acf_series.index > 1]  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–∞–≥ 0 –∏ 1
                            if not acf_series.empty:
                                best_lag = int(acf_series.iloc[acf_series.abs().argmax()].name)
                                best_val = float(acf_series.loc[best_lag])
                                if abs(best_val) >= 0.3:
                                    st.success(
                                        f"–í–æ–∑–º–æ–∂–Ω—ã–π —Å–µ–∑–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥: **{best_lag}** —à–∞–≥–æ–≤ (ACF‚âà{best_val:.2f}). "
                                        "–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –ø–µ—Ä–∏–æ–¥ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –≤ STL –∏–ª–∏ ARIMA."
                                    )
                                else:
                                    st.info("–Ø–≤–Ω–æ –≤—ã—Ä–∞–∂–µ–Ω–Ω–æ–≥–æ —Å–µ–∑–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –≤ ACF –Ω–µ –≤–∏–¥–Ω–æ.")
                else:
                    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ACF.")

        else:
            if date_col != "<–Ω–µ—Ç>":
                st.info("–ù—É–∂–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.")

    # ---- OUTLIERS
    with tab_outliers:
        st.header("4Ô∏è‚É£ –ü–æ–∏—Å–∫ –≤—ã–±—Ä–æ—Å–æ–≤")
        if not numeric_cols:
            st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")
        else:
            out_col = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞", numeric_cols)
            with st.expander("IQR-–≤—ã–±—Ä–æ—Å—ã", expanded=True):
                if st.button("–ù–∞–π—Ç–∏ –≤—ã–±—Ä–æ—Å—ã (IQR)", help="IQR ‚Äî –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö; –≤—ã–±—Ä–æ—Å–∞–º–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è —Ç–æ—á–∫–∏ –¥–∞–ª–µ–∫–æ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ [Q1 - 1.5¬∑IQR, Q3 + 1.5¬∑IQR]."):
                    mask, lower, upper, iqr = detect_outliers_iqr(df[out_col], k=1.5)
                    n = int(mask.sum())
                    c1, c2, c3 = st.columns(3)
                    c1.metric("–í—ã–±—Ä–æ—Å–æ–≤", n)
                    c2.metric("% —Å—Ç—Ä–æ–∫", f"{(n/len(df)*100 if len(df) else 0):.2f}%")
                    c3.metric("IQR", f"{iqr:.3g}" if not np.isnan(iqr) else "n/a")
                    if n > 0:
                        out_df = df[mask].copy()
                        with st.expander("–¢–∞–±–ª–∏—Ü–∞ –≤—ã–±—Ä–æ—Å–æ–≤", expanded=False):
                            st.dataframe(out_df.head(50))
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã (CSV)", data=out_df.to_csv(index=False).encode("utf-8"),
                                           file_name=f"outliers_iqr_{out_col}.csv", mime="text/csv")

            with st.expander("Z-score –≤—ã–±—Ä–æ—Å—ã", expanded=False):
                zt = st.slider(
                    "–ü–æ—Ä–æ–≥ |Z|",
                    min_value=1.0,
                    max_value=6.0,
                    value=3.0,
                    step=0.5,
                    help="Z‚Äëscore ‚Äî —á–∏—Å–ª–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ; –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ—Ä–æ–≥ 3.0 –¥–ª—è –≥—Ä—É–±—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ –∏ 2.0 –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞."
                )
                if st.button("–ù–∞–π—Ç–∏ –≤—ã–±—Ä–æ—Å—ã (Z-score)"):
                    mask = detect_outliers_z(df[out_col], z_thresh=zt)
                    n = int(mask.sum())
                    st.metric("–í—ã–±—Ä–æ—Å–æ–≤", n)
                    if n > 0:
                        out_df = df[mask].copy()
                        st.dataframe(out_df.head(50))
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã (CSV)", data=out_df.to_csv(index=False).encode("utf-8"),
                                           file_name=f"outliers_z_{out_col}.csv", mime="text/csv")

            # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –≤—ã–±—Ä–æ—Å–∞–º + AI –ø–æ –∫–Ω–æ–ø–∫–µ
            try:
                mask_iqr, lower_iqr, upper_iqr, iqr_val = detect_outliers_iqr(df[out_col], k=1.5)
                n_iqr = int(mask_iqr.sum())
                mask_z_default = detect_outliers_z(df[out_col], z_thresh=3.0)
                n_z_default = int(mask_z_default.sum())
                parts = [
                    f"–í—ã–±—Ä–æ—Å—ã –ø–æ –∫–æ–ª–æ–Ω–∫–µ {out_col}: IQR‚Äë–∫—Ä–∏—Ç–µ—Ä–∏–π —Å k=1.5 –¥–∞—ë—Ç {n_iqr} —Ç–æ—á–µ–∫ "
                    f"(–¥–∏–∞–ø–∞–∑–æ–Ω [{lower_iqr:.3g}; {upper_iqr:.3g}]).",
                    f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Z‚Äëscore —Å –ø–æ—Ä–æ–≥–æ–º 3.0 –æ—Ç–º–µ—á–∞–µ—Ç {n_z_default} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∫–∞–∫ –≤—ã–±—Ä–æ—Å—ã.",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏ –≤–≤–æ–¥–∞ –∏, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫–ª–∏–ø–ø–∏–Ω–≥ –∏–ª–∏ –∏–º–ø—É—Ç–∞—Ü–∏—é."
                ]
                local_out_summary = " ".join(parts)
            except Exception:
                local_out_summary = f"–ü–æ –∫–æ–ª–æ–Ω–∫–µ {out_col} –≤—ã–±—Ä–æ—Å—ã —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã, —Å–µ—Ä—å—ë–∑–Ω—ã—Ö –æ—à–∏–±–æ–∫ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."

            render_ai_block(
                local_out_summary,
                "ü§ñ AI-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ –≤—ã–±—Ä–æ—Å–∞–º",
                f"outliers_{out_col}",
                extra_prompt="–ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –º–∞—Å—à—Ç–∞–±—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –≤—ã–±—Ä–æ—Å–∞–º–∏, –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–ª–∏–ø–ø–∏–Ω–≥, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –∏–º–ø—É—Ç–∞—Ü–∏—è)."
            )

            # --- –ò–º–ø—É—Ç–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
            with st.expander("–ò–º–ø—É—Ç–∞—Ü–∏—è / –æ—á–∏—Å—Ç–∫–∞", expanded=False):
                # –≤—ã–±–∏—Ä–∞–µ–º, –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã –±—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
                cols_imp = st.multiselect("–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", numeric_cols, key="imp_cols")
                method_imp = st.selectbox("–ú–µ—Ç–æ–¥", ["median", "most_frequent", "winsorize"], key="imp_method")
                if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å", key="imp_apply"):
                    if not cols_imp:
                        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∫–æ–ª–æ–Ω–∫—É.")
                    else:
                        tmp = df.copy()
                        # –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                        for cc in cols_imp:
                            if method_imp == "median":
                                try:
                                    med = tmp[cc].median()
                                    tmp[cc] = tmp[cc].fillna(med)
                                except Exception:
                                    pass
                            elif method_imp == "most_frequent":
                                try:
                                    m = tmp[cc].mode()
                                    if not m.empty:
                                        tmp[cc] = tmp[cc].fillna(m.iloc[0])
                                except Exception:
                                    pass
                            else:  # winsorize
                                try:
                                    tmp[cc] = winsorize_series(tmp[cc], 0.01, 0.99)
                                except Exception:
                                    pass
                        st.dataframe(tmp.head(50))
                        st.download_button(
                            "üì• –°–∫–∞—á–∞—Ç—å (CSV)",
                            data=tmp.to_csv(index=False).encode("utf-8"),
                            file_name="cleaned_data.csv",
                            mime="text/csv",
                        )

    # ---- FEATURES
    with tab_feats:
        st.header("5Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ñ–∏—á–µ–π")
        with st.expander("–§–∏—á–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", expanded=False):
            dcol = st.selectbox("–î–∞—Ç–∞-–∫–æ–ª–æ–Ω–∫–∞", ["<–Ω–µ—Ç>"] + analysis_cols, key="fe_dt")
            if dcol != "<–Ω–µ—Ç>" and numeric_cols:
                vcol = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="fe_val")
                win = st.slider("–û–∫–Ω–æ –¥–ª—è rolling", 3, 60, 7, 1, key="fe_win")
                spike = st.slider("–ü–æ—Ä–æ–≥ –≤—Å–ø–ª–µ—Å–∫–∞ |pct_change|, %", 5.0, 300.0, 50.0, 5.0, key="fe_spike")
                if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏—á–∏", key="fe_btn"):
                    feats, g = generate_ts_features(df, dcol, vcol, window=win, spike_thresh_pct=spike)
                    st.json(g)
                    st.dataframe(feats.head(20))
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ñ–∏—á–∏ (CSV)", data=feats.to_csv(index=False).encode("utf-8"),
                                       file_name=f"ts_features_{vcol}.csv", mime="text/csv")
            elif dcol != "<–Ω–µ—Ç>":
                st.info("–ù—É–∂–Ω—ã —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã.")

        st.markdown("---")
        st.subheader("–¢–∞–±–ª–∏—á–Ω—ã–µ —Ñ–∏—á–∏")
        with st.expander("–ë–∏–Ω–Ω–∏–Ω–≥ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", expanded=False):
            if numeric_cols:
                bcol = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞", numeric_cols, key="bin_col")
                method = st.radio("–ú–µ—Ç–æ–¥", ["qcut (–ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º)", "cut (—Ä–∞–≤–Ω–∞—è —à–∏—Ä–∏–Ω–∞)"], horizontal=True, key="bin_m")
                n_bins = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–Ω–æ–≤", 3, 10, 5, 1, key="bin_n")
                new_name = st.text_input("–ò–º—è –Ω–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏", value=f"{bcol}_bin_{n_bins}", key="bin_name")
                if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∏–Ω—ã", key="bin_btn"):
                    try:
                        binned = (pd.qcut(df[bcol], q=n_bins, duplicates="drop") if method.startswith("qcut")
                                  else pd.cut(df[bcol], bins=n_bins))
                        tmp = df.copy(); tmp[new_name] = binned
                        st.dataframe(tmp[[bcol, new_name]].head(30))
                        st.download_button("üì• –°–∫–∞—á–∞—Ç—å (CSV)", data=tmp.to_csv(index=False).encode("utf-8"),
                                           file_name=f"binned_{new_name}.csv", mime="text/csv")
                    except Exception as e:
                        st.error(f"–ë–∏–Ω–Ω–∏–Ω–≥ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω: {e}")
            else:
                st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")

        with st.expander("–§–ª–∞–≥–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º", expanded=False):
            if numeric_cols:
                tcol = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞", numeric_cols, key="thr_col")
                ttype = st.radio("–¢–∏–ø", ["> X", "< X"], horizontal=True, key="thr_type")
                tval = st.number_input("–ü–æ—Ä–æ–≥ X", value=float(df[tcol].median()) if len(df[tcol].dropna()) else 0.0, key="thr_val")
                fname = st.text_input("–ò–º—è —Ñ–ª–∞–≥–∞", value=f"{tcol}_flag", key="thr_name")
                if st.button("–°–æ–∑–¥–∞—Ç—å —Ñ–ª–∞–≥", key="thr_btn"):
                    tmp = df.copy()
                    tmp[fname] = (tmp[tcol] > tval).astype(int) if ttype == "> X" else (tmp[tcol] < tval).astype(int)
                    st.dataframe(tmp[[tcol, fname]].head(30))
                    st.download_button("üì• –°–∫–∞—á–∞—Ç—å (CSV)", data=tmp.to_csv(index=False).encode("utf-8"),
                                       file_name=f"flag_{fname}.csv", mime="text/csv")
            else:
                st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")

        with st.expander("–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ Z-score", expanded=False):
            num_multi = st.multiselect("–ö–æ–ª–æ–Ω–∫–∏", numeric_cols, key="logz_cols")
            if num_multi and st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å", key="logz_btn"):
                tmp = df.copy()
                for c in num_multi:
                    tmp[f"{c}_log1p"] = np.log1p(tmp[c])
                    std = tmp[c].std(ddof=0)
                    tmp[f"{c}_z"] = (tmp[c] - tmp[c].mean()) / std if std and not np.isnan(std) else np.nan
                st.dataframe(tmp.head(20))
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å (CSV)", data=tmp.to_csv(index=False).encode("utf-8"),
                                   file_name="transformed_features.csv", mime="text/csv")

    # ---- CATEGORICAL
    with tab_cats:
        st.header("6Ô∏è‚É£ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
        if not categorical_cols:
            st.info("–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")
        else:
            cat = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞", categorical_cols)
            with st.expander("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π", expanded=True):
                vc = df[cat].astype(str).value_counts(dropna=False)
                total = int(vc.sum())
                freq_df = vc.rename("count").to_frame(); freq_df["share_%"] = freq_df["count"] / total * 100.0
                st.dataframe(freq_df.head(50))
                top = freq_df.head(20).reset_index().rename(columns={"index": cat})
                fig = px.bar(top, x=cat, y="count", title=f"–¢–æ–ø-20 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ {cat}"); fig.update_xaxes(tickangle=60)
                st.plotly_chart(fig, use_container_width=True)
            with st.expander("–ú–µ—Ç—Ä–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", expanded=False):
                if numeric_cols:
                    m = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="cat_metric")
                    agg = st.selectbox("–ê–≥—Ä–µ–≥–∞—Ç", ["mean", "median", "sum", "count", "std"], key="cat_agg")
                    grouped = df.groupby(cat)[m].agg(agg).sort_values(ascending=False).rename(agg)
                    st.dataframe(grouped.head(50))
                    top_g = grouped.head(20).reset_index()
                    fig2 = px.bar(top_g, x=cat, y=agg, title=f"{m} –ø–æ {cat}"); fig2.update_xaxes(tickangle=60)
                    st.plotly_chart(fig2, use_container_width=True)
                else:
                    st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")

    # ---- GROUPS / PIVOT
    with tab_groups:
        st.header("7Ô∏è‚É£ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø –∏ —Å–≤–æ–¥–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã")
        if categorical_cols and numeric_cols:
            with st.expander("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø (groupby)", expanded=True):
                gcol = st.selectbox("–ì—Ä—É–ø–ø–∞", categorical_cols, key="grp_col")
                mcol = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="grp_metric")
                agg = st.selectbox("–ê–≥—Ä–µ–≥–∞—Ç", ["mean", "median", "sum", "count", "std"], key="grp_agg")
                grouped = df.groupby(gcol)[mcol].agg(agg).sort_values(ascending=False).rename(agg)
                st.dataframe(grouped.head(100))
                top_g = grouped.head(20).reset_index()
                st.plotly_chart(px.bar(top_g, x=gcol, y=agg, title=f"–¢–æ–ø-20 {gcol} –ø–æ {mcol}"), use_container_width=True)
        else:
            st.info("–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≥—Ä—É–ø–ø –Ω—É–∂–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –∏ —á–∏—Å–ª–æ–≤—ã–µ.")

        st.markdown("---")
        st.subheader("üìä Pivot / —Å–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞")
        if categorical_cols and numeric_cols:
            row_col = st.selectbox("–°—Ç—Ä–æ–∫–∏ (rows)", categorical_cols, key="pivot_row")
            col_col = st.selectbox("–°—Ç–æ–ª–±—Ü—ã (columns, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", ["<–Ω–µ—Ç>"] + categorical_cols, key="pivot_col")
            val_col = st.selectbox("–ó–Ω–∞—á–µ–Ω–∏–µ (value)", numeric_cols, key="pivot_val")
            agg_pivot = st.selectbox("–ê–≥—Ä–µ–≥–∞—Ü–∏—è", ["mean", "sum", "count", "median", "std"], key="pivot_agg")

            with st.expander("–î–æ–ø. —Ñ–∏–ª—å—Ç—Ä—ã (–ø–æ –ª—é–±—ã–º –∫–æ–ª–æ–Ω–∫–∞–º)", expanded=False):
                extra_cols = st.multiselect("–ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –¥–æ–ø. —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", options=df.columns.tolist(), key="pivot_extra_cols")
                extra_filters = {}
                for c in extra_cols:
                    s = df[c]
                    if np.issubdtype(s.dtype, np.number):
                        mn, mx = float(s.min()), float(s.max())
                        if not (np.isnan(mn) or np.isnan(mx)):
                            extra_filters[c] = ("range", st.slider(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è {c}", mn, mx, (mn, mx), key=f"pf_rng_{c}"))
                    else:
                        vals = sorted(s.dropna().astype(str).unique())
                        chosen = st.multiselect(f"–ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {c}", options=vals, key=f"pf_vals_{c}")
                        if chosen: extra_filters[c] = ("values", chosen)

            with st.expander("–§–∏–ª—å—Ç—Ä—ã –ø–æ rows/columns", expanded=False):
                row_vals = st.multiselect(f"–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º {row_col}", options=sorted(df[row_col].dropna().astype(str).unique()), key="pf_row_vals")
                col_vals = st.multiselect(f"–§–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º {col_col}", options=sorted(df[col_col].dropna().astype(str).unique()), key="pf_col_vals") if col_col != "<–Ω–µ—Ç>" else []

            chart_type = st.selectbox("–¢–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞", [
                "–ë–µ–∑ –≥—Ä–∞—Ñ–∏–∫–∞",
                "Heatmap",
                "Bar (rows, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π)",
                "Bar (rows, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π)",
                "Bar (columns, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π)",
                "Bar (columns, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π)",
                "Line (rows)",
                "Line (columns)",
                "Stacked bar (rows √ó columns)",
                "Stacked bar (columns √ó rows)",
                "Treemap (rows)",
            ], key="pivot_chart")

            if st.button("–ü–æ—Å—Ç—Ä–æ–∏—Ç—å pivot"):
                d = df.copy()
                if row_vals: d = d[d[row_col].astype(str).isin(row_vals)]
                if col_col != "<–Ω–µ—Ç>" and col_vals: d = d[d[col_col].astype(str).isin(col_vals)]
                for c, (kind, val) in extra_filters.items():
                    if kind == "range":
                        lo, hi = val; d = d[(d[c] >= lo) & (d[c] <= hi)]
                    else:
                        d = d[d[c].astype(str).isin(val)]
                columns_arg = None if col_col == "<–Ω–µ—Ç>" else col_col
                pvt = pd.pivot_table(d, index=row_col, columns=columns_arg, values=val_col, aggfunc=agg_pivot)
                st.dataframe(pvt)

                if chart_type != "–ë–µ–∑ –≥—Ä–∞—Ñ–∏–∫–∞" and pvt.size > 0:
                    p = pvt.to_frame("value").reset_index() if isinstance(pvt, pd.Series) else pvt.copy()

                    if chart_type == "Heatmap":
                        if pvt.shape[0] <= 50 and (1 if isinstance(pvt, pd.Series) else pvt.shape[1]) <= 50:
                            st.plotly_chart(px.imshow(pvt if not isinstance(pvt, pd.Series) else pvt.to_frame("value"),
                                                      aspect="auto", color_continuous_scale="Blues",
                                                      title=f"Heatmap: {agg_pivot}({val_col})"),
                                            use_container_width=True)
                        else:
                            st.caption("–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è heatmap.")

                    elif chart_type in ["Bar (rows, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π)", "Bar (rows, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π)"]:
                        series_rows = (pvt.sum(axis=1) if not isinstance(pvt, pd.Series) and pvt.shape[1] > 1 else pvt.squeeze())
                        df_bar = series_rows.reset_index(); df_bar.columns = [row_col, "value"]
                        fig = px.bar(df_bar, x=row_col, y="value", title=f"{agg_pivot}({val_col}) –ø–æ {row_col}")
                        if chart_type.endswith("–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π"):
                            fig.update_traces(orientation="h"); fig.update_yaxes(categoryorder="total ascending")
                        fig.update_xaxes(tickangle=60); st.plotly_chart(fig, use_container_width=True)

                    elif chart_type in ["Bar (columns, –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π)", "Bar (columns, –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π)"] and columns_arg is not None:
                        series_cols = pvt.sum(axis=0)
                        df_bar = series_cols.reset_index(); df_bar.columns = [col_col, "value"]
                        fig = px.bar(df_bar, x=col_col, y="value", title=f"{agg_pivot}({val_col}) –ø–æ {col_col}")
                        if chart_type.endswith("–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π"):
                            fig.update_traces(orientation="h"); fig.update_yaxes(categoryorder="total ascending")
                        fig.update_xaxes(tickangle=60); st.plotly_chart(fig, use_container_width=True)

                    elif chart_type == "Line (rows)":
                        series_rows = pvt.sum(axis=1)
                        df_line = series_rows.reset_index(); df_line.columns = [row_col, "value"]
                        st.plotly_chart(px.line(df_line, x=row_col, y="value", markers=True,
                                                title=f"–õ–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –ø–æ {row_col}"), use_container_width=True)

                    elif chart_type == "Line (columns)" and columns_arg is not None:
                        series_cols = pvt.sum(axis=0)
                        df_line = series_cols.reset_index(); df_line.columns = [col_col, "value"]
                        st.plotly_chart(px.line(df_line, x=col_col, y="value", markers=True,
                                                title=f"–õ–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –ø–æ {col_col}"), use_container_width=True)

                    elif chart_type == "Stacked bar (rows √ó columns)" and columns_arg is not None:
                        df_melt = pvt.reset_index().melt(id_vars=[row_col], var_name=col_col, value_name="value")
                        st.plotly_chart(px.bar(df_melt, x=row_col, y="value", color=col_col,
                                               title=f"Stacked: {row_col} √ó {col_col}"), use_container_width=True)

                    elif chart_type == "Stacked bar (columns √ó rows)" and columns_arg is not None:
                        df_melt = pvt.reset_index().melt(id_vars=[row_col], var_name=col_col, value_name="value")
                        st.plotly_chart(px.bar(df_melt, x=col_col, y="value", color=row_col,
                                               title=f"Stacked: {col_col} √ó {row_col}"), use_container_width=True)

                    elif chart_type == "Treemap (rows)":
                        series_rows = (pvt.sum(axis=1) if not isinstance(pvt, pd.Series) and pvt.shape[1] > 1 else pvt.squeeze())
                        df_tree = series_rows.reset_index(); df_tree.columns = [row_col, "value"]
                        st.plotly_chart(px.treemap(df_tree, path=[row_col], values="value", title=f"Treemap –ø–æ {row_col}"),
                                        use_container_width=True)
        else:
            st.info("–î–ª—è —Å–≤–æ–¥–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –Ω—É–∂–Ω–∞ —Ö–æ—Ç—è –±—ã 1 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∏ 1 —á–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞.")

    # ---- A/B
    with tab_ab:
        st.header("8Ô∏è‚É£ A/B —Ç–µ—Å—Ç")
        if categorical_cols and numeric_cols:
            with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏", expanded=True):
                gcol = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞ –≥—Ä—É–ø–ø", categorical_cols, key="ab_col")
                levels = sorted(df[gcol].dropna().astype(str).unique().tolist())
                if len(levels) < 2:
                    st.info("–ù—É–∂–Ω–æ ‚â•2 —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è."); gA=gB=None
                else:
                    gA = st.selectbox("–ì—Ä—É–ø–ø–∞ A", levels, key="ab_A")
                    gB = st.selectbox("–ì—Ä—É–ø–ø–∞ B", [v for v in levels if v != gA], key="ab_B") if len(levels)>1 else None
                m = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="ab_m")
                test_kind = st.selectbox("–¢–∏–ø —Ç–µ—Å—Ç–∞", ["t-test (Welch)", "t-test (equal var)", "Mann‚ÄìWhitney U", "z-test (proportions)"])
                alpha = st.number_input(
                    "Œ±",
                    min_value=0.001,
                    max_value=0.2,
                    value=0.05,
                    step=0.005,
                    help=(
                        "–£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ Œ±: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—à–∏–±–æ—á–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –Ω—É–ª–µ–≤–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã. "
                        "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 0.05."
                    ),
                )

            if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç"):
                if not gA or not gB:
                    st.warning("–í—ã–±–µ—Ä–∏—Ç–µ 2 –≥—Ä—É–ø–ø—ã.")
                else:
                    x = df.loc[df[gcol].astype(str)==gA, m].dropna()
                    y = df.loc[df[gcol].astype(str)==gB, m].dropna()
                    if len(x)<2 or len(y)<2:
                        st.info("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π.")
                    else:
                        mean_a, mean_b, diff = float(x.mean()), float(y.mean()), float(y.mean() - x.mean())
                        d_val = cohen_d(x, y)
                        if test_kind == "Mann‚ÄìWhitney U":
                            # –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
                            u, p = stats.mannwhitneyu(x, y, alternative="two-sided")
                            st.subheader("Mann‚ÄìWhitney U")
                            st.write({"u_stat": float(u), "p_value": float(p)})
                        elif test_kind == "z-test (proportions)":
                            # –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–Ω–∏—Ü—É –¥–æ–ª–µ–π (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –º–µ—Ç—Ä–∏–∫—É, 0/1)
                            try:
                                # –î–æ–ª–∏ —É—Å–ø–µ—Ö–æ–≤
                                p1 = x.mean() if len(x) else np.nan
                                p2 = y.mean() if len(y) else np.nan
                                p_hat = (x.sum() + y.sum()) / (len(x) + len(y)) if (len(x) + len(y)) > 0 else np.nan
                                se = math.sqrt(p_hat * (1 - p_hat) * (1 / len(x) + 1 / len(y))) if (len(x) > 0 and len(y) > 0) else np.nan
                                z_stat = (p2 - p1) / se if se and not np.isnan(se) else np.nan
                                p = 2 * (1 - stats.norm.cdf(abs(z_stat))) if not np.isnan(z_stat) else np.nan
                                st.subheader("z-test (proportions)")
                                st.write({"z_stat": float(z_stat) if not np.isnan(z_stat) else None, "p_value": float(p) if not np.isnan(p) else None})
                            except Exception as _:
                                p = np.nan
                        else:
                            # t-—Ç–µ—Å—Ç—ã (Welch –∏–ª–∏ —Å —Ä–∞–≤–Ω—ã–º–∏ –¥–∏—Å–ø–µ—Ä—Å–∏—è–º–∏)
                            equal_var = (test_kind == "t-test (equal var)")
                            t, p = stats.ttest_ind(x, y, equal_var=equal_var)
                            st.subheader("t-test")
                            st.write({"t_stat": float(t), "p_value": float(p), "cohen_d": float(d_val) if not np.isnan(d_val) else None})
                        c1, c2, c3, c4 = st.columns(4)
                        c1.metric(f"Mean A ({gA})", f"{mean_a:.3g}"); c2.metric(f"Mean B ({gB})", f"{mean_b:.3g}")
                        c3.metric("Œî (B-A)", f"{diff:.3g}"); c4.metric("p-value", f"{p:.4f}")
                        local_ab_summary = business_summary_for_ab(gA, gB, mean_a, mean_b, diff, p, alpha, d_val)
                        render_ai_block(
                            local_ab_summary,
                            "ü§ñ AI-–±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ A/B-—Ç–µ—Å—Ç—É",
                            f"ab_main_{gA}_{gB}",
                            extra_prompt=(
                                "–°–¥–µ–ª–∞–π –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç—Ç–æ–≥–æ A/B-—Ç–µ—Å—Ç–∞. "
                                "–û–ø–∏—à–∏, –µ—Å—Ç—å –ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ –æ—Ç–ª–∏—á–∏—è, –∫–∞–∫–æ–≤ —Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ "
                                "–∏ –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–∂–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –ø—Ä–æ–¥–∞–∫—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä—É."
                            ),
                        )
        else:
            st.info("–ù—É–∂–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –∏ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.")

    # ---- SCENARIOS
    with tab_scenarios:
        st.header("9Ô∏è‚É£ –°—Ü–µ–Ω–∞—Ä–∏–∏ / —à–∞–±–ª–æ–Ω—ã")
        with st.expander("–ë—ã—Å—Ç—Ä—ã–π EDA –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º", expanded=True):
            if numeric_cols:
                eda_cols = st.multiselect("–ú–µ—Ç—Ä–∏–∫–∏", numeric_cols, default=numeric_cols[:min(3, len(numeric_cols))])
                if eda_cols:
                    target_metric = st.selectbox("–û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ", eda_cols, index=0)
                else:
                    target_metric = None

                if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π EDA"):
                    if not eda_cols:
                        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏.")
                    else:
                        # 1) –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        stats_df = pd.DataFrame({c: describe_basic_stats(df[c]) for c in eda_cols}).T
                        with st.expander("1) –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏", expanded=True):
                            st.dataframe(stats_df)

                        # 2) –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                        corr_matrix = None
                        if len(eda_cols) >= 2:
                            # –ö—ç—à–∏—Ä—É–µ–º —Ä–∞—Å—á—ë—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                            corr_matrix = compute_corr_matrix_cached(df, tuple(eda_cols))
                            with st.expander("2) –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏", expanded=False):
                                st.plotly_chart(
                                    px.imshow(
                                        corr_matrix,
                                        text_auto=False,
                                        color_continuous_scale="RdBu",
                                        zmin=-1,
                                        zmax=1,
                                        title="–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (–±—ã—Å—Ç—Ä—ã–π EDA)",
                                    ),
                                    use_container_width=True,
                                )

                        # 3) –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
                        dq = compute_data_quality_table(df[eda_cols])
                        with st.expander("3) –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö", expanded=False):
                            st.dataframe(dq)

                        # 4) –ê–≤—Ç–æ-—Å–≤–æ–¥–∫–∞ –ø–æ EDA (–ª–æ–∫–∞–ª—å–Ω–∞—è + AI –ø–æ –∫–Ω–æ–ø–∫–µ)
                        local_summary = auto_eda_summary(df, stats_df, corr_matrix, dq, eda_cols)
                        with st.expander("4) –ê–≤—Ç–æ-—Å–≤–æ–¥–∫–∞ –ø–æ EDA", expanded=True):
                            render_ai_block(
                                local_summary,
                                "ü§ñ AI-—Å–≤–æ–¥–∫–∞ –ø–æ EDA",
                                "scenario_eda_global",
                                extra_prompt=(
                                    "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö "
                                    "—Å–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫—É—é —Å–≤–æ–¥–∫—É –¥–ª—è –ø—Ä–æ–¥–∞–∫—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞. "
                                    "–û–ø–∏—à–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏, –∞–Ω–æ–º–∞–ª–∏–∏ –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏."
                                ),
                            )

                        # 5) –ë–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
                        if target_metric is not None:
                            metric_stats = stats_df.loc[target_metric].to_dict()
                            local_metric_summary = business_summary_for_series(
                                target_metric,
                                metric_stats,
                                norm_res=None,
                                n_outliers=0,
                            )
                            with st.expander("5) üß† –ë–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –º–µ—Ç—Ä–∏–∫–µ", expanded=True):
                                render_ai_block(
                                    local_metric_summary,
                                    "ü§ñ AI-—Ä–µ–∑—é–º–µ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ",
                                    f"scenario_metric_{target_metric}",
                                    extra_prompt=(
                                        f"–°–¥–µ–ª–∞–π –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –º–µ—Ç—Ä–∏–∫–µ '{target_metric}' –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
                                        "–û–ø–∏—à–∏, –∫–∞–∫ –µ—ë –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç/–±–∏–∑–Ω–µ—Å, –∫–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å."
                                    ),
                                )

                        # –î–ª—è HTML-–æ—Ç—á—ë—Ç–∞ –±–µ—Ä—ë–º AI-—Å–≤–æ–¥–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–Ω–∞, –∏–Ω–∞—á–µ –ª–æ–∫–∞–ª—å–Ω—É—é
                        summary_for_report = local_summary
                        if "ai_summaries" in st.session_state:
                            summary_for_report = st.session_state["ai_summaries"].get("scenario_eda_global", local_summary)

                        report_bytes = build_auto_eda_html(
                            df, eda_cols, stats_df, corr_matrix, dq, summary_for_report
                        )
                        st.download_button(
                            "üì• –°–∫–∞—á–∞—Ç—å –∞–≤—Ç–æ-EDA –æ—Ç—á—ë—Ç (HTML)",
                            data=report_bytes,
                            file_name="auto_eda_report.html",
                            mime="text/html",
                        )
            else:
                st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è EDA.")
        with st.expander("–ë—ã—Å—Ç—Ä—ã–π A/B (–¥–≤–µ –∫—Ä—É–ø–Ω–µ–π—à–∏–µ –≥—Ä—É–ø–ø—ã)", expanded=False):
            if categorical_cols and numeric_cols:
                gcol = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞ –≥—Ä—É–ø–ø"
, categorical_cols, key="sc_g")
                m = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="sc_m")
                levels = df[gcol].astype(str).value_counts().index.tolist()
                if len(levels) >= 2:
                    gA, gB = levels[0], levels[1]
                    st.caption(f"A={gA}, B={gB}")
                    if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π A/B"):
                        x = df.loc[df[gcol].astype(str)==gA, m].dropna()
                        y = df.loc[df[gcol].astype(str)==gB, m].dropna()
                        if len(x)>=2 and len(y)>=2:
                            mean_a, mean_b, diff = float(x.mean()), float(y.mean()), float(y.mean()-x.mean())
                            t, p = stats.ttest_ind(x, y, equal_var=False); d = cohen_d(x, y)
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric(f"Mean A ({gA})", f"{mean_a:.3g}"); c2.metric(f"Mean B ({gB})", f"{mean_b:.3g}")
                            c3.metric("Œî (B-A)", f"{diff:.3g}"); c4.metric("p-value", f"{p:.4f}")
                            st.write({"t_stat": float(t), "p_value": float(p), "cohen_d": float(d) if not np.isnan(d) else None})

                            # –õ–æ–∫–∞–ª—å–Ω–æ–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ –±—ã—Å—Ç—Ä–æ–º—É A/B + AI –ø–æ –∫–Ω–æ–ø–∫–µ
                            local_ab_summary = business_summary_for_ab(gA, gB, mean_a, mean_b, diff, p, 0.05, d)
                            render_ai_block(
                                local_ab_summary,
                                "ü§ñ AI-—Ä–µ–∑—é–º–µ –ø–æ –±—ã—Å—Ç—Ä–æ–º—É A/B",
                                f"ab_quick_{gcol}_{m}",
                                extra_prompt=(
                                    "–°–¥–µ–ª–∞–π –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç—Ç–æ–≥–æ A/B-—Ç–µ—Å—Ç–∞. "
                                    "–û–ø–∏—à–∏, –µ—Å—Ç—å –ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ –æ—Ç–ª–∏—á–∏—è, –∫–∞–∫–æ–≤ —Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ "
                                    "–∏ –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–∂–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –ø—Ä–æ–¥–∞–∫—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä—É."
                                ),
                            )
                        else:
                            st.info("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π.")
                else:
                    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥—Ä—É–ø–ø.")
            else:
                st.info("–ù—É–∂–Ω—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –∏ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.")

        # --- –ù–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ / —à–∞–±–ª–æ–Ω—ã ---
        with st.expander("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏–∏", expanded=False):
            """–ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ–π –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏. –í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É,
            –º–µ—Ç—Ä–∏–∫—É –∏ –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã; –¥–∞–ª–µ–µ –≤—ã–±–µ—Ä–∏—Ç–µ –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ
            —Ä—è–¥—ã. –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–æ–∂–µ—Ç —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ–≤–∞—Ç—å –æ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏–∏."""
            if categorical_cols and numeric_cols:
                cannib_cat = st.selectbox(
                    "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞",
                    categorical_cols,
                    key="cannib_cat",
                )
                cannib_metric = st.selectbox(
                    "–ú–µ—Ç—Ä–∏–∫–∞",
                    numeric_cols,
                    key="cannib_metric",
                )
                cannib_date = st.selectbox(
                    "–ö–æ–ª–æ–Ω–∫–∞ –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏",
                    ["<–Ω–µ—Ç>"] + analysis_cols,
                    key="cannib_date",
                )
                if cannib_date != "<–Ω–µ—Ç>":
                    categories = df[cannib_cat].dropna().astype(str).unique().tolist()
                    if len(categories) >= 2:
                        cat1 = st.selectbox("–ö–∞—Ç–µ–≥–æ—Ä–∏—è 1", categories, key="cannib_cat1")
                        cat2_opts = [c for c in categories if c != cat1]
                        cat2 = st.selectbox("–ö–∞—Ç–µ–≥–æ—Ä–∏—è 2", cat2_opts, key="cannib_cat2") if cat2_opts else None
                        if cat2:
                            if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏–∏", key="btn_cannib"):
                                try:
                                    d = df[[cannib_date, cannib_cat, cannib_metric]].dropna().copy()
                                    d[cannib_date] = pd.to_datetime(d[cannib_date])
                                    s1 = (
                                        d[d[cannib_cat] == cat1]
                                        .groupby(cannib_date)[cannib_metric]
                                        .sum()
                                        .rename(cat1)
                                    )
                                    s2 = (
                                        d[d[cannib_cat] == cat2]
                                        .groupby(cannib_date)[cannib_metric]
                                        .sum()
                                        .rename(cat2)
                                    )
                                    joined = pd.concat([s1, s2], axis=1).dropna()
                                    if len(joined) >= 2:
                                        corr_val = joined[cat1].corr(joined[cat2])
                                        st.line_chart(joined)
                                        st.metric("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è (r)", f"{corr_val:.3g}")
                                        st.caption(
                                            "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏—é."
                                        )
                                    else:
                                        st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
                                except Exception as e:
                                    st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
                    else:
                        st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.")
                else:
                    st.info("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–Ω–Ω–∏–±–∞–ª–∏–∑–∞—Ü–∏–∏.")
            else:
                st.info("–ù—É–∂–Ω–∞ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è –∏ –æ–¥–Ω–∞ —á–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞.")

        with st.expander("–í—ã—è–≤–ª–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ (ACF/STL)", expanded=False):
            """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏ STL-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è. –í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç—É,
            –º–µ—Ç—Ä–∏–∫—É –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∞–≥. –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–∫–∞–∂–µ—Ç ACF –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –≤–æ–∑–º–æ–∂–Ω—ã–π —Å–µ–∑–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥."""
            if numeric_cols:
                seas_date = st.selectbox(
                    "–ö–æ–ª–æ–Ω–∫–∞ –¥–∞—Ç—ã",
                    ["<–Ω–µ—Ç>"] + analysis_cols,
                    key="seas_date",
                )
                seas_value = st.selectbox("–ú–µ—Ç—Ä–∏–∫–∞", numeric_cols, key="seas_value")
                seas_maxlag = st.slider(
                    "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∞–≥ –¥–ª—è ACF",
                    min_value=5,
                    max_value=100,
                    value=30,
                    step=1,
                    key="seas_maxlag",
                )
                if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å", key="btn_seas"):
                    if seas_date == "<–Ω–µ—Ç>":
                        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã.")
                    else:
                        try:
                            d = df[[seas_date, seas_value]].dropna().copy()
                            d[seas_date] = pd.to_datetime(d[seas_date])
                            d = d.sort_values(seas_date)
                            lags, acf_vals = compute_acf(d[seas_value], seas_maxlag)
                            fig = go.Figure()
                            fig.add_trace(go.Bar(x=lags, y=acf_vals, name="ACF"))
                            fig.update_layout(
                                title="–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è",
                                xaxis_title="Lag",
                                yaxis_title="ACF",
                                height=400,
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Å–µ–∑–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–∏—Å–∫–ª—é—á–∞–µ–º –ª–∞–≥ 0)
                            if len(acf_vals) > 1:
                                best_idx = int(np.argmax(np.abs(acf_vals[1:])) + 1)
                                best_lag = int(lags[best_idx])
                                st.write(f"–í–æ–∑–º–æ–∂–Ω—ã–π —Å–µ–∑–æ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥: {best_lag}")
                            # STL-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –≤—ã–±–æ—Ä—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                            if STL is not None and st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å STL-—Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ", key="seas_stl"):
                                try:
                                    comp = STL(d[seas_value], period=best_lag if len(acf_vals) > 1 else seas_maxlag).fit()
                                    comp_df = pd.DataFrame(
                                        {
                                            "date": d[seas_date].values,
                                            "observed": comp.observed,
                                            "trend": comp.trend,
                                            "seasonal": comp.seasonal,
                                            "resid": comp.resid,
                                        }
                                    )
                                    fig_stl = make_stl_figure(comp_df)
                                    st.plotly_chart(fig_stl, use_container_width=True)
                                except Exception as e:
                                    st.error(f"–û—à–∏–±–∫–∞ STL: {e}")
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ ACF: {e}")
            else:
                st.info("–ù—É–∂–Ω–∞ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —á–∏—Å–ª–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞.")

        with st.expander("–í–ª–∏—è–Ω–∏–µ –∞–∫—Ü–∏–π / —Å–æ–±—ã—Ç–∏–π", expanded=False):
            """–û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∞–∫—Ü–∏–∏ –∏–ª–∏ —Å–æ–±—ã—Ç–∏—è: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ –¥–æ –∏ –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã."
            """
            if numeric_cols:
                event_date_col = st.selectbox(
                    "–ö–æ–ª–æ–Ω–∫–∞ –¥–∞—Ç—ã",
                    ["<–Ω–µ—Ç>"] + analysis_cols,
                    key="event_date_col",
                )
                event_value = st.selectbox(
                    "–ú–µ—Ç—Ä–∏–∫–∞",
                    numeric_cols,
                    key="event_value_col",
                )
                event_dt = st.date_input(
                    "–î–∞—Ç–∞ —Å–æ–±—ã—Ç–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ/–ø–æ—Å–ª–µ)",
                    key="event_dt",
                )
                if st.button("–û—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ", key="btn_event"):
                    if event_date_col == "<–Ω–µ—Ç>":
                        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã.")
                    else:
                        try:
                            d = df[[event_date_col, event_value]].dropna().copy()
                            d[event_date_col] = pd.to_datetime(d[event_date_col])
                            before = d[d[event_date_col] < pd.to_datetime(event_dt)][event_value]
                            after = d[d[event_date_col] >= pd.to_datetime(event_dt)][event_value]
                            if len(before) >= 2 and len(after) >= 2:
                                mean_before = float(before.mean())
                                mean_after = float(after.mean())
                                diff = mean_after - mean_before
                                t_stat, p_val = stats.ttest_ind(after, before, equal_var=False)
                                c1, c2, c3, c4 = st.columns(4)
                                c1.metric("–°—Ä–µ–¥–Ω–µ–µ –¥–æ", f"{mean_before:.3g}")
                                c2.metric("–°—Ä–µ–¥–Ω–µ–µ –ø–æ—Å–ª–µ", f"{mean_after:.3g}")
                                c3.metric("Œî (–ø–æ—Å–ª–µ - –¥–æ)", f"{diff:.3g}")
                                c4.metric("p-value", f"{p_val:.4f}")
                                if p_val < 0.05:
                                    st.success("–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ.")
                                else:
                                    st.info("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.")
                            else:
                                st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –¥–∞—Ç—ã.")
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            else:
                st.info("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

    # ---- DICTIONARY
    with tab_dict:
        st.header("üîü –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤")
        key = st.selectbox("–ú–µ—Ç–æ–¥", list(METHOD_INFO.keys()), format_func=lambda k: METHOD_INFO[k]["name"])
        info = METHOD_INFO[key]
        st.subheader(info["name"]); st.markdown(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {info['description']}"); st.markdown(f"**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** {info['when']}")


    # ---- REPORT
    with tab_report:
        st.header("1Ô∏è‚É£1Ô∏è‚É£ –û—Ç—á—ë—Ç—ã –∏ —ç–∫—Å–ø–æ—Ä—Ç")

        if df is None or df.empty:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á—ë—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —à–∞–≥–∞—Ö 1‚Äì2.")
        else:
            st.subheader("üîß –ö–æ–Ω—Ñ–∏–≥ –æ—Ç—á—ë—Ç–∞")

            # –ö–∞–∫–∏–µ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∫–ª—é—á–∞—Ç—å –≤ EDA-—á–∞—Å—Ç—å
            num_cols = df.select_dtypes(include="number").columns.tolist()
            report_num_cols = st.multiselect(
                "–ß–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞ (EDA, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏, AI-—Å–≤–æ–¥–∫–∞)",
                num_cols,
                default=num_cols,
                key="report_num_cols",
            )

            if not report_num_cols:
                st.info("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —á–∏—Å–ª–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ—Ç—á—ë—Ç–∞.")
            else:
                cols = report_num_cols
                stats_df = pd.DataFrame({c: describe_basic_stats(df[c]) for c in cols}).T

                corr_matrix = None
                if len(cols) >= 2:
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                        corr_matrix = compute_corr_matrix_cached(df, tuple(cols))
                    except Exception:
                        corr_matrix = None

                dq = compute_data_quality_table(df[cols])

                # –¢–µ–∫—Å—Ç–æ–≤–∞—è –∞–≤—Ç–æ-—Å–≤–æ–¥–∫–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è) + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è AI-–≤–µ—Ä—Å–∏—è –ø–æ –∫–Ω–æ–ø–∫–µ
                summary_text = auto_eda_summary(df, stats_df, corr_matrix, dq, cols)
                st.subheader("üìÑ –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞")
                st.markdown(f"<div class='business-summary'>{summary_text}</div>", unsafe_allow_html=True)

                with st.expander("ü§ñ AI-—Å–≤–æ–¥–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–∞", expanded=False):
                    render_ai_block(
                        summary_text,
                        "ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±—â—É—é AI-—Å–≤–æ–¥–∫—É –¥–ª—è –æ—Ç—á—ë—Ç–∞",
                        "report_global",
                        extra_prompt=(
                            "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∑—é–º–µ –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É. "
                            "–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ä–∏—Å–∫–∞—Ö, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö —Ä–æ—Å—Ç–∞ –∏ –≥–∏–ø–æ—Ç–µ–∑–∞—Ö –¥–ª—è A/B-—Ç–µ—Å—Ç–æ–≤."
                        ),
                    )

                # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–π–¥—ë—Ç –≤ –æ—Ç—á—ë—Ç—ã: AI, –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ª–æ–∫–∞–ª—å–Ω—ã–π
                summary_for_report = summary_text
                if "ai_summaries" in st.session_state:
                    summary_for_report = st.session_state["ai_summaries"].get("report_global", summary_text)

                # --- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤ ---
                # HTML‚Äë–æ—Ç—á—ë—Ç (–∞–≤—Ç–æ‚ÄëEDA)
                html_bytes = build_auto_eda_html(
                    df,
                    cols,
                    stats_df,
                    corr_matrix,
                    dq,
                    summary_text=summary_for_report,
                )
                st.download_button(
                    "üì• –°–∫–∞—á–∞—Ç—å HTML-–æ—Ç—á—ë—Ç (EDA + AI-—Å–≤–æ–¥–∫–∞)",
                    data=html_bytes,
                    file_name="auto_eda_ai_report.html",
                    mime="text/html",
                )

                # Excel‚Äë–æ—Ç—á—ë—Ç (Data + EDA + DQ + AI). –°–æ–∑–¥–∞—ë—Ç—Å—è –ø–æ –Ω–∞–∂–∞—Ç–∏—é –∫–Ω–æ–ø–∫–∏.
                report_xlsx: BytesIO | None = None
                report_pptx: BytesIO | None = None
                if st.button("–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç—ã (Excel + PPTX)", key="btn_build_excel_report"):
                    with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç—ã..."):
                        text_blocks = {
                            "EDA summary (rule-based)": summary_text,
                            "Global AI summary": summary_for_report,
                        }
                        report_xlsx = build_excel_report(
                            df,
                            stats_df=stats_df,
                            corr_matrix=corr_matrix,
                            dq=dq,
                            text_blocks=text_blocks,
                        )
                        # –°—Ç—Ä–æ–∏–º PPTX-–æ—Ç—á—ë—Ç
                        report_pptx = build_pptx_report(
                            df,
                            stats_df=stats_df,
                            corr_matrix=corr_matrix,
                            dq=dq,
                            summary_text=summary_for_report,
                        )
                    if report_xlsx:
                        st.download_button(
                            "üì• –°–∫–∞—á–∞—Ç—å Excel-–æ—Ç—á—ë—Ç",
                            data=report_xlsx,
                            file_name="stats_lab_report_v5_6.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        )
                    if report_pptx and report_pptx.getbuffer().nbytes > 0:
                        st.download_button(
                            "üì• –°–∫–∞—á–∞—Ç—å PPTX-–æ—Ç—á—ë—Ç",
                            data=report_pptx,
                            file_name="stats_lab_report_v5_6.pptx",
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        )

                # One‚Äëclick –æ—Ç—á—ë—Ç: zip HTML, Excel –∏ PPTX, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
                if html_bytes and report_xlsx:
                    try:
                        import zipfile
                        bundle = BytesIO()
                        with zipfile.ZipFile(bundle, mode="w") as zf:
                            zf.writestr("report.html", html_bytes)
                            # Excel
                            if hasattr(report_xlsx, "getvalue"):
                                zf.writestr("report.xlsx", report_xlsx.getvalue())
                            else:
                                zf.writestr("report.xlsx", report_xlsx)
                            # PPTX
                            if report_pptx and hasattr(report_pptx, "getvalue") and report_pptx.getbuffer().nbytes > 0:
                                zf.writestr("report.pptx", report_pptx.getvalue())
                        bundle.seek(0)
                        st.download_button(
                            "üì• One‚Äëclick –æ—Ç—á—ë—Ç (HTML + Excel + PPTX)",
                            data=bundle.getvalue(),
                            file_name="stats_lab_report_bundle.zip",
                            mime="application/zip",
                        )
                    except Exception:
                        pass

                # –ü—Ä–æ—Ñ–∏–ª—å –∞–Ω–∞–ª–∏–∑–∞: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                profile_dict = {"filters": filter_config, "selected_report_columns": cols}
                profile_bytes = json.dumps(profile_dict, ensure_ascii=False, indent=2).encode("utf-8")
                st.download_button(
                    "üì• –°–∫–∞—á–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –∞–Ω–∞–ª–∏–∑–∞ (JSON)",
                    data=profile_bytes,
                    file_name="analysis_profile.json",
                    mime="application/json",
                )

                # –û—Ç—á—ë—Ç —Ç–æ–ª—å–∫–æ —Å –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ DataSheet)
                report = build_excel_report(df, None, None, None, None)
                st.download_button(
                    "üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á—ë—Ç Excel",
                    data=report,
                    file_name="stats_report_v5_6.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )


if __name__ == "__main__":
    main()